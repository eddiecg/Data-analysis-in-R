[["introduction-to-programming-in-r.html", "Fundamentals of Data Analysis in R Chapter 1 Introduction to Programming in R 1.1 Calculations in R 1.2 Variable assignation in R 1.3 Data types in R 1.4 Working with multiple numbers in R 1.5 Manipulating data in R 1.6 Extending R’s funcitonality with libraries 1.7 Writing data from R 1.8 Reading data into R 1.9 Project work", " Fundamentals of Data Analysis in R Eddie Cano-Gamez 2022-08-15 Chapter 1 Introduction to Programming in R R is a programming language designed to read, write and manipulate data. It is especially suitable for performing statistical tests and modelling data. This session illustrates some of the most important tasks which can be performed in R. 1.1 Calculations in R The easiest way to use R is as a simple calculator. For example, you can calculate the sum, difference or product of two or more numbers in R as you would in any other calculator: 2+2 ## [1] 4 3-1 ## [1] 2 3*5 ## [1] 15 6/3 ## [1] 2 However, R is also able to perform more complex computations, such as those in a scientific calculator. For instance, one can use R to calculate roots, logarithms or results from exponential operations. This is done via R “functions”. Functions have a name (called the function), followed by two parentheses. Inside these parentheses lie the arguments of the function. For example, the command “sqrt(9)” applies the sqrt function (which calculates the squared root) to its argument (number 9): sqrt(9) ## [1] 3 Some functions accept more than one argument. For example, the function “log” (which calculates logarithms) accepts two arguments: the number we are aplying logarithm to, and the base of the logarithm: log(100, base=10) calculates the log10 of 100. log(x=100, base=10) ## [1] 2 Arguments can be specified in two ways: positional: R knows which argument is which because they appear in a certain order explicit: We specify the name of the argument follwed by an equality sign (=) and the desired value. For example, the following command has the same result as before: calculating the log10 of 100. However, we do not need to specify which is x and which is base, since R will immediately infer this from their position (x always goes first and base always goes second). log(100,10) ## [1] 2 If you do not know what a function does or how to use it, you can access the “help” information using a question mark: ?log This helps exists for any R function. 1.2 Variable assignation in R Most of the times, we will need to use R as something more than just a scientific computer: we might want to keep track of the things we have done and store our results in the computer memory (RAM). R allows us to do that by assigning values to variables. In R variables are always object. We analyse see what this means below, but for now we should remember that a variable has three characteristics: A name: We can call our variables with whichever name we want, with only three exceptions: 1.1 Variable names cannot start with a number, they need to start with a letter 1.2 Variable names cannot start with a dot 1.3 You should not name the variables with any “protected name”. A protected name is a word which by default already means something in R. The most common example are the letters F and T, which in R mean TRUE and FALSE, two logical values 1.4 Variable names are case sensitive: ST, St, sT and st will be four different variables A type: variables in R can be text, numbers or qualitative variables. We will discuss this in more detail below. A value: this is the actual piece of information we are “setting” that variable to be. To assign a value to a variable we use the following operator: &lt;- These two symbols (&lt;-) represent an arrow pointing from right to left and in English mean “Let’s assign the value in the right to the word in the left”. For example: A &lt;- log(356,2) Means “Let’s set A to be the log2 of 356” (log2 of 356 is simply a number, close to 8.5). Now this number has been stored in RAM under the name “A”. We can access it by simply tying A in the console: A ## [1] 8.475733 Because it is so common in other languages, now R also accepts the following syntax, which means the exact same: A = log(356, 2) A ## [1] 8.475733 1.3 Data types in R As mentioned above, in R information is stored in the form of objects (variables) which can belong to several data types. Let’s take a look at the different data types available in R 1.3.1 Numeric variables The simplest data types are those which consist of a single element. This element can, for example, be a number. Numbers in R are called “numeric variables”. There are two types of numbers: integers (non-fractional) and fractional numbers. In computers, fractional numbers are stored as “Floating point numbers”, which is a type of scientific notation. Our previous variable “A”” is a number (specifically, a floating point number). To verify this, we can use the function class(), which tells us the data type of a variable. class(A) ## [1] &quot;numeric&quot; If we want to transform this number into an integer, we can use the function as.integer() as follows: as.integer(A) ## [1] 8 Note that now we don’t get 8.4757… but only 8. To transform the fractional number into a integer, R has chopped all the numbers after the decimal point. Let’s look at the class of this new object: class(as.integer(A)) ## [1] &quot;integer&quot; We see that it is no longer a “numeric” variable, but an “integer”. 1.3.2 Character variables Sometimes we will need to work with text. For example, we might want to store a series of words. Texts in R are “character” variables. To tell R that something should be a character variable we put it in quotes: B &lt;- &quot;word&quot; Let’s look at the value and clas of B: class(B) ## [1] &quot;character&quot; B ## [1] &quot;word&quot; B is a character variable with value “word”. Be careful of always having the correct data type! Often errors in R come from thinking we have a number when we actually have a text, or vice versa. For examle, the following variable contains number 2, but in a text format: A &lt;- &quot;2&quot; class(A) ## [1] &quot;character&quot; A ## [1] &quot;2&quot; If we try to add it two number 2 we will get an error, since we are trying to add a character variable to a numeric variable A + 2 We can convert characters to numbers using the function as.numeric() or viceversa using the function as.character(). For instance, let’s convert A to a numeric format: A &lt;- as.numeric(A) If we try to add 2 to this number, we will see that the error has disappeared: A + 2 ## [1] 4 Inside the computer memory, R character variables are stored as ASCII symbols. 1.3.3 Logical variables Often we will need to know if something is true or false. For example, is our variable “A” bigger than 3? A &gt; 3 ## [1] FALSE Or, is our variable a number? is.numeric(A) ## [1] TRUE The answer to all these questions comes in a binary format: YES or NO, TRUE or FALSE. These binary (boolean) values are stored in R as “Logical” variables. The importance of logical variables will become apparent as we learn more about R. 1.3.4 Categorical variables (factors) Sometimes our data will not consists of numbers of words, but rather of categories or groups. For example, we might have data on female and male individuals: which of them is which? Or we might have data on people from Mexico, the UK and France: which of them is which? This type of information is stored in R as “factors”. To create a factor, we use the function factor(). For example, the following command creates a list of factors for individuals from three nationalities: two are Mexican, 3 British and 2 French. Don’t pay much attention to the c() operator yet, this will be explained below. A &lt;- factor(c(&quot;MEX&quot;, &quot;UK&quot;, &quot;UK&quot;, &quot;FR&quot;, &quot;MEX&quot;, &quot;FR&quot;, &quot;UK&quot;)) A ## [1] MEX UK UK FR MEX FR UK ## Levels: FR MEX UK Notice how the factor has two components: the levels and the values. This is because factors are actually stored as numbers (integers), not as words! For example, in this case R created the following equivalence: Word Number FR 1 MEX 2 UK 3 So that each Frenck individual is given a number 1, each Mexican a number 2 and each British a number 3. Numbers are easier to store than words! R ordered our labels simply alphabetically. We can verify this with the mode function: mode(A) ## [1] &quot;numeric&quot; Indeed, our data is made of numbers! In fact, we can obtain this numbers using as.numeric(): as.numeric(A) ## [1] 2 3 3 1 2 1 3 On the other hand, we can easily transform this to words using as.character(): as.character(A) ## [1] &quot;MEX&quot; &quot;UK&quot; &quot;UK&quot; &quot;FR&quot; &quot;MEX&quot; &quot;FR&quot; &quot;UK&quot; More generally, the levels of a factor can be accessed using the “levels()” function: levels(A) ## [1] &quot;FR&quot; &quot;MEX&quot; &quot;UK&quot; Factors are extremely important in statistics, because they allow us to split our data in groups and look for differencs between them. 1.4 Working with multiple numbers in R Single numbers or words are not very interesting, and they will hardly ever be the centre of an R analysis. What we really need is a way to store tens, hundreds or thousands of numbers and words. In R, this can actually be done very easily using objects called “vectors”. 1.4.1 Vectors A series of values can be stored in a single variable in the form of a vector. The easiest way to define a vector in R is using the “c()” operator (“c” stands for create or catenate) as follows: vec &lt;- c(1,2,3,4,5) Since all of the members of our vector are numbers, the class of this vector is numeric: vec ## [1] 1 2 3 4 5 class(vec) ## [1] &quot;numeric&quot; Vectors also have an atribute called “length” (number of elements in the vector) which can be accessed with the length() operator: length(vec) ## [1] 5 To retrieve specific elements from a vector, we can use the [] operator. Inside the brackets we should include the index (position) of the element we want to access. R always indexes its objects with base 1 (as opposed to base 0), which means that the first element with have index 1, the second index 2, and so forth. To access the second element of our vector, we do the following: vec[2] ## [1] 2 If we want to create a vector of consecutive numbers (a sequence) without typing each of them, we can use the colon “:” operator, as follows: 1:5 ## [1] 1 2 3 4 5 The same result can be achieved using the seq() function. When using seq() we need to specify the starting and ending point of the sequence: seq(1,10) ## [1] 1 2 3 4 5 6 7 8 9 10 It is possible to add a third argument to the seq() function, which specifies the interval/increment size of the sequence. For example, the following function generates a sequence of numbers from 1 to 10 in increments of 2: seq(1,10,2) ## [1] 1 3 5 7 9 Vectors can also store character variables. For example, we can define a vector which contains animal names: vec2 &lt;- c(A=&quot;dog&quot;,B=&quot;cat&quot;,C=&quot;rabbit&quot;) vec2 ## A B C ## &quot;dog&quot; &quot;cat&quot; &quot;rabbit&quot; The class of this vector will be “character”: class(vec2) ## [1] &quot;character&quot; Note that when I defined the vector I assigned a name to each of its elements (in this case A, B, and C). The name of a vector’s elements can be accessed using the names() function: names(vec2) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; A named element in a vector can be accessed directly by its name: vec2[&quot;C&quot;] ## C ## &quot;rabbit&quot; Names can also be assigned after the vector has been created. For example, let’s have a look at our first vector. This vector does not contain any names: vec ## [1] 1 2 3 4 5 names(vec) ## NULL We can assign names to it using the names() function and the ‘&lt;-’ operator as follows: names(vec) &lt;- c(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;,&quot;four&quot;,&quot;five&quot;) Now each element of vec is indexed by a name: vec ## one two three four five ## 1 2 3 4 5 vec[&quot;two&quot;] ## two ## 2 Sometimes we will need to create vectors in which a number (or a group of numbers) is repeated over and over again. This can be easily done using the rep() function (“rep” standing for replicate). For example, the following function will create a vector that contains the values 1 and 2, repeated 5 times: rep(c(1,2),5) ## [1] 1 2 1 2 1 2 1 2 1 2 rep() can also be used to replicate the content of a vector. For example, the following line creates a vector containing three copies o the content of vec2: rep(vec2,3) ## A B C A B C A B C ## &quot;dog&quot; &quot;cat&quot; &quot;rabbit&quot; &quot;dog&quot; &quot;cat&quot; &quot;rabbit&quot; &quot;dog&quot; &quot;cat&quot; &quot;rabbit&quot; 1.4.2 Lists Often we will need to store elements of different classes into a single variable. For instance, imagine you want to store numbers and text in the same variable. You can try to assign them to a vector as follows: myvector &lt;- c(1,3,6:3,&quot;cat&quot;,&quot;dog&quot;) However, the result from this operation is not what we expected: myvector ## [1] &quot;1&quot; &quot;3&quot; &quot;6&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;cat&quot; &quot;dog&quot; The quotes indicate us that all the variables are text! In fact, R has converted all the numbers to characters: class(myvector) ## [1] &quot;character&quot; This is because a vector can only store values which all belong to the same class. If we want to store elements of multiple data types in a single variable, we use a “list”. Lists are defined using the list() function: mylist &lt;- list(A=1,B=3,C=6:3,D=&quot;cat&quot;,E=&quot;dog&quot;) Note how each element is stored independently and can be of a different class and have different dimentions. In this case, the element “C” is itself a vector of length 4. mylist ## $A ## [1] 1 ## ## $B ## [1] 3 ## ## $C ## [1] 6 5 4 3 ## ## $D ## [1] &quot;cat&quot; ## ## $E ## [1] &quot;dog&quot; If the list contains names, then each element can be accessed by its name using the $ sign: mylist$D ## [1] &quot;cat&quot; This is the same as accessing the element using its index: mylist[[4]] ## [1] &quot;cat&quot; Note that for lists you have to use the double bracket operator [[]] instead of the single bracket used for vectors. Each of the objects in a list can have different classes and modes: class(mylist$A) ## [1] &quot;numeric&quot; class(mylist$C) ## [1] &quot;integer&quot; class(mylist$D) ## [1] &quot;character&quot; 1.4.3 Matrices Sometimes even vectors and lists will not be enough for storing data. This is the case when one deals with tables of data or matrices. For example, we might have information on “cancer samples” and “healthy samples” from a hospital, and some measurements on each of them. We will want to store at least two variables: The cancer and healthy labels (this can be, for instance, a factor) The measurements To manipulate this kind of data structures we can use matrices and data frames. A matrix object consists of “n” rows and “m” columns. To create a matrix in R simply use the “matrix()” function and specify the data you want to fill the matrix with. For example, this line creates a sequence of numbers from 1 to 10 and stores them in a matrix format: matrix(1:10) ## [,1] ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 ## [5,] 5 ## [6,] 6 ## [7,] 7 ## [8,] 8 ## [9,] 9 ## [10,] 10 Note that by default matrix() assumes that you only want one column. If you need the data to be stored in multiple columns, simply specify the number of rows and columns you need: matrix(1:10,nrow=5,ncol=2) ## [,1] [,2] ## [1,] 1 6 ## [2,] 2 7 ## [3,] 3 8 ## [4,] 4 9 ## [5,] 5 10 Importantly, R always assumes you want to arrange the data in a matrix “by column”. This means, starting from the top left and going down the first column, then the second one, and so forth. If you need the data to be allocated row by row, simply set the byrow argument to TRUE. matrix(1:100,nrow=10,ncol=10,byrow=TRUE) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 2 3 4 5 6 7 8 9 10 ## [2,] 11 12 13 14 15 16 17 18 19 20 ## [3,] 21 22 23 24 25 26 27 28 29 30 ## [4,] 31 32 33 34 35 36 37 38 39 40 ## [5,] 41 42 43 44 45 46 47 48 49 50 ## [6,] 51 52 53 54 55 56 57 58 59 60 ## [7,] 61 62 63 64 65 66 67 68 69 70 ## [8,] 71 72 73 74 75 76 77 78 79 80 ## [9,] 81 82 83 84 85 86 87 88 89 90 ## [10,] 91 92 93 94 95 96 97 98 99 100 Just as vectors have names, the columns and rows of a matrix can also be labelled. These labels can be specified using the “dimnames” argument. dimnames has to be a list with two elements, the name of rows and the name of columns. Let’s craete a 100 x 5 matrix containing numbers from 1 to 500. We name the rows of the matrix with numbers and the columns with letters. mat &lt;- matrix(1:500,nrow=100,ncol=5, dimnames=list(1:100,c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;))) The funciton “head()” displays the top elements of the matrix head(mat) ## A B C D E ## 1 1 101 201 301 401 ## 2 2 102 202 302 402 ## 3 3 103 203 303 403 ## 4 4 104 204 304 404 ## 5 5 105 205 305 405 ## 6 6 106 206 306 406 The function “tail()” displays the bottom elements tail(mat) ## A B C D E ## 95 95 195 295 395 495 ## 96 96 196 296 396 496 ## 97 97 197 297 397 497 ## 98 98 198 298 398 498 ## 99 99 199 299 399 499 ## 100 100 200 300 400 500 You can use “colnames()” and “rownames()” to access the row and column names of a matrix: colnames(mat) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; rownames(mat) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;20&quot; &quot;21&quot; &quot;22&quot; &quot;23&quot; &quot;24&quot; &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;28&quot; ## [29] &quot;29&quot; &quot;30&quot; &quot;31&quot; &quot;32&quot; &quot;33&quot; &quot;34&quot; &quot;35&quot; &quot;36&quot; &quot;37&quot; &quot;38&quot; &quot;39&quot; &quot;40&quot; &quot;41&quot; &quot;42&quot; &quot;43&quot; &quot;44&quot; &quot;45&quot; &quot;46&quot; &quot;47&quot; &quot;48&quot; &quot;49&quot; &quot;50&quot; &quot;51&quot; &quot;52&quot; &quot;53&quot; &quot;54&quot; &quot;55&quot; &quot;56&quot; ## [57] &quot;57&quot; &quot;58&quot; &quot;59&quot; &quot;60&quot; &quot;61&quot; &quot;62&quot; &quot;63&quot; &quot;64&quot; &quot;65&quot; &quot;66&quot; &quot;67&quot; &quot;68&quot; &quot;69&quot; &quot;70&quot; &quot;71&quot; &quot;72&quot; &quot;73&quot; &quot;74&quot; &quot;75&quot; &quot;76&quot; &quot;77&quot; &quot;78&quot; &quot;79&quot; &quot;80&quot; &quot;81&quot; &quot;82&quot; &quot;83&quot; &quot;84&quot; ## [85] &quot;85&quot; &quot;86&quot; &quot;87&quot; &quot;88&quot; &quot;89&quot; &quot;90&quot; &quot;91&quot; &quot;92&quot; &quot;93&quot; &quot;94&quot; &quot;95&quot; &quot;96&quot; &quot;97&quot; &quot;98&quot; &quot;99&quot; &quot;100&quot; Finally, it is possible to access specific elements in a matrix using either their names or their indexes. Note that both indexes (the column and row number) need to be specified. mat[1,4] ## [1] 301 mat[1,&quot;D&quot;] ## [1] 301 If the column number is left empty, then R retrieves all the elements in that row (and vice versa). mat[1,] ## A B C D E ## 1 101 201 301 401 For instances, the following line retrieves all elements in the column labelled “D” and calculates the average (mean): mean(mat[,&quot;D&quot;]) ## [1] 350.5 Matrices are very useful, since they allow the user to perform operations to each of its rows or columns. Another way of building matrices is by concatenating multiple vectors. The vectors in question have to be of the same length. Let’s create two different vectors with length 10 each: A &lt;- seq(1:10) B &lt;- seq(11:20) Now let’s combine them. We can do this using the “cbind()” function (cbind standing for “column binding”). cbind(A,B) ## A B ## [1,] 1 1 ## [2,] 2 2 ## [3,] 3 3 ## [4,] 4 4 ## [5,] 5 5 ## [6,] 6 6 ## [7,] 7 7 ## [8,] 8 8 ## [9,] 9 9 ## [10,] 10 10 One can verify that this new object is a matrix class(cbind(A,B)) ## [1] &quot;matrix&quot; &quot;array&quot; The columns of this matrix are named after the names of each individual vector colnames(cbind(A,B)) ## [1] &quot;A&quot; &quot;B&quot; Vectors can also be used as two rows of a matrix, insted of two columns. To do this, we use rbind(), which stands for “row binding”. rbind(A,B) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## A 1 2 3 4 5 6 7 8 9 10 ## B 1 2 3 4 5 6 7 8 9 10 1.4.4 Data frames As with vectors, often we will need to store elements of different classes into a single matrix. For instance, imagine you want to store a list of names along with an identifier for each name. Let’s try to store this data as a matrix: First we generate a vector of IDs, then a vector of names: ID &lt;- 1:6 Name &lt;- c(&quot;Jimmy&quot;,&quot;Amanda&quot;,&quot;Glenn&quot;,&quot;Toby&quot;,&quot;Ren&quot;,&quot;Amanda&quot;) Finally, we bind both columns: names &lt;- cbind(ID,Name) Note that everything has been transformed to a character form (hence the quotes): names ## ID Name ## [1,] &quot;1&quot; &quot;Jimmy&quot; ## [2,] &quot;2&quot; &quot;Amanda&quot; ## [3,] &quot;3&quot; &quot;Glenn&quot; ## [4,] &quot;4&quot; &quot;Toby&quot; ## [5,] &quot;5&quot; &quot;Ren&quot; ## [6,] &quot;6&quot; &quot;Amanda&quot; This is because, just as a vector, a matrix can only store values which all belong to the same class. To store objets of several different classes in a single object, we need to build a data frame. To create a data frame in R simply use the data.frame() function as follows: names &lt;- data.frame(ID,Name) names ## ID Name ## 1 1 Jimmy ## 2 2 Amanda ## 3 3 Glenn ## 4 4 Toby ## 5 5 Ren ## 6 6 Amanda Note how now each column belongs to a different class: class(names$ID) ## [1] &quot;integer&quot; class(names$Name) ## [1] &quot;character&quot; However, the names have been stored as factors instead of characters. This is because R always assumes that the text in a data.frame represents factors. To keep them as characters instead, simply set the stringsAsFactors paramteres to FALSE: names &lt;- data.frame(ID,Name, stringsAsFactors=FALSE) Now we have a numeric and a character column: class(names$ID) ## [1] &quot;integer&quot; class(names$Name) ## [1] &quot;character&quot; Data frames are useful because it is easy to perform operations on each row or column individually. For example, we can now use “table()” to tabulate the names and find out if any of them is repeated more than once. table(names$Name) ## ## Amanda Glenn Jimmy Ren Toby ## 2 1 1 1 1 1.5 Manipulating data in R As a programming language, R has been designed to optimally work with vectors, lists, matrices and data frames, as will be illustrated in the following examples. Let’s create a data frame containing the name of 6 individuals as well as their height, body mass index (BMI), age and hours of sleep. Since we do not have access to such dataset, we “simulate it” using a “random” number generator. We will not discuss random number generation in R just now, but we’ll return to it later. Let’s first define a vector of names: names &lt;- c(&quot;Jimmy&quot;,&quot;Amanda&quot;,&quot;Glenn&quot;,&quot;Toby&quot;,&quot;Ren&quot;,&quot;Amanda&quot;) Next, we generate 6 random numbers that represent height in centimetres (a height for each individual). To do this we use rnorm(), which is a function that generates random numbers with mean (average) of 165 cm and standard deviation (a measure of spread which we will learn about later) of 10cm. heights &lt;- rnorm(6,mean=165,sd=10) We repeat this process for BMI and sleep hours (each of them will have its own mean and standard deviaiton). BMIs &lt;- rnorm(6,mean=27,sd=2) sleep &lt;- rnorm(6,mean=7,sd=2) Now, we create a vector of ages. In this case, we simply invent the age of our fictional characters: age &lt;- c(21,23,23,40,19,35) Finally, we combine everything into a single data.frame object: dat &lt;- data.frame(names,heights,BMIs,age,sleep) Let’s have a look at it: dat ## names heights BMIs age sleep ## 1 Jimmy 183.1423 28.16605 21 8.424872 ## 2 Amanda 165.0718 24.00776 23 7.344862 ## 3 Glenn 148.0559 24.59611 23 8.067157 ## 4 Toby 180.7543 26.73520 40 7.626297 ## 5 Ren 162.6167 27.78964 19 6.636333 ## 6 Amanda 168.0388 28.41036 35 5.209633 Using the function “dim()” we verify the dimensions of this data set dim(dat) ## [1] 6 5 Let’s now look closer into a family of functions called “apply”. These functions are designed to repeat an operation across all elements of a data frame, list or vector. In order to use apply(), we specify the following arguments: X = matrix or data frame where our data is stored MARGIN = whether we want the operation to be repeated per row (1) or per column (2) FUN = operation we want to perform In this case, we want to find out the class of each of the columns in the data frame “dat”: apply(X=dat, MARGIN=2, FUN=class) ## names heights BMIs age sleep ## &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; Now we combine apply() with function “Summary”, which takes a group of numbers and summarises them in six parameters: minimum, mean, median, maximum and interquartile ranges. apply(X=dat[,2:5], MARGIN=2, FUN=summary) ## heights BMIs age sleep ## Min. 148.0559 24.00776 19.00000 5.209633 ## 1st Qu. 163.2304 25.13088 21.50000 6.813465 ## Median 166.5553 27.26242 23.00000 7.485579 ## Mean 167.9466 26.61752 26.83333 7.218192 ## 3rd Qu. 177.5754 28.07195 32.00000 7.956942 ## Max. 183.1423 28.41036 40.00000 8.424872 1.5.1 Working with lists of data frames Data frames can themselves be grouped into lists or higher-order structures. To illustrate this, let’s create a new fictional dataset of heights, BMIs, ages and sleep hours. Now let’s also include the variable sex. We will simulate randm data for 10,000 individuals (Yes, then thousand!) and will name each individual with a numeric ID instead of a name (coming up with 10000 fictional names would be too time consuming and require too much imagination!): IDs &lt;- seq(1,10000) heights &lt;- rnorm(10000,mean=165,sd=10) BMIs &lt;- rnorm(10000,mean=27,sd=2) ages &lt;- round(rnorm(10000,mean=25,sd=10)) sex &lt;- sample(x=c(0,1),size=10000,replace=TRUE) sleep &lt;- rnorm(10000,mean=7,sd=2) dat2 &lt;- data.frame(IDs,heights,BMIs,ages,sex,sleep) Let’s compare our first, smaller data set with the new data. dat ## names heights BMIs age sleep ## 1 Jimmy 183.1423 28.16605 21 8.424872 ## 2 Amanda 165.0718 24.00776 23 7.344862 ## 3 Glenn 148.0559 24.59611 23 8.067157 ## 4 Toby 180.7543 26.73520 40 7.626297 ## 5 Ren 162.6167 27.78964 19 6.636333 ## 6 Amanda 168.0388 28.41036 35 5.209633 head(dat2) ## IDs heights BMIs ages sex sleep ## 1 1 167.1295 23.48994 24 0 5.694760 ## 2 2 166.7796 26.02868 31 0 2.543716 ## 3 3 163.8697 24.82922 34 0 9.108687 ## 4 4 166.7887 28.74675 33 1 1.875554 ## 5 5 174.0567 28.80829 16 1 2.501058 ## 6 6 159.3937 23.51663 34 0 8.314209 Under certain circumstances we will want to store both datasets together. For instance, maybe they are data from two different populations which we later want to compare. We can do this by creating a list of data frames: database &lt;- list(A=dat,B=dat2) Here, the element A of our list is the first data set, and the element B the second: head(database$A) ## names heights BMIs age sleep ## 1 Jimmy 183.1423 28.16605 21 8.424872 ## 2 Amanda 165.0718 24.00776 23 7.344862 ## 3 Glenn 148.0559 24.59611 23 8.067157 ## 4 Toby 180.7543 26.73520 40 7.626297 ## 5 Ren 162.6167 27.78964 19 6.636333 ## 6 Amanda 168.0388 28.41036 35 5.209633 head(database$B) ## IDs heights BMIs ages sex sleep ## 1 1 167.1295 23.48994 24 0 5.694760 ## 2 2 166.7796 26.02868 31 0 2.543716 ## 3 3 163.8697 24.82922 34 0 9.108687 ## 4 4 166.7887 28.74675 33 1 1.875554 ## 5 5 174.0567 28.80829 16 1 2.501058 ## 6 6 159.3937 23.51663 34 0 8.314209 Now we can use another function of the apply family called “lapply” to repeat an operation in all the elements of a list (in this case, the list of data frames). We can combine lapply() with apply() to calculate the summary numbers of each column o both datasets. Let’s store the results in a new variable: results &lt;- lapply(database, function(l){ apply(X=l[,2:dim(l)[2]], MARGIN=2, FUN=summary) }) Note that the results variable is itself a list (since lapply generates a list output): results ## $A ## heights BMIs age sleep ## Min. 148.0559 24.00776 19.00000 5.209633 ## 1st Qu. 163.2304 25.13088 21.50000 6.813465 ## Median 166.5553 27.26242 23.00000 7.485579 ## Mean 167.9466 26.61752 26.83333 7.218192 ## 3rd Qu. 177.5754 28.07195 32.00000 7.956942 ## Max. 183.1423 28.41036 40.00000 8.424872 ## ## $B ## heights BMIs ages sex sleep ## Min. 127.8211 19.59680 -12.0000 0.0000 -0.5597662 ## 1st Qu. 158.2283 25.70933 18.0000 0.0000 5.6587163 ## Median 165.0022 27.00980 25.0000 0.0000 6.9829551 ## Mean 164.9724 27.03470 25.0168 0.4981 6.9772389 ## 3rd Qu. 171.6043 28.36951 32.0000 1.0000 8.3210660 ## Max. 201.4211 34.21441 65.0000 1.0000 14.3471767 class(results) ## [1] &quot;list&quot; 1.6 Extending R’s funcitonality with libraries Sometimes the basic functions in R are not enough for the type of analysis we are interested in. In these cases, we can expand R’s functionality by installing additional groups of functions called “libraries”. A large proportion of R libraries are stored in the “Comprehensive R Archive Network” (CRAN) and can be installed using the “install.packages()” function followed by the name of the library. The following line installs the libraries “rafalib” and “reshape2” from CRAN: install.packages(&quot;rafalib&quot;) install.packages(&quot;reshape2&quot;) rafalib contains a group of functions which facilitate data exploration and visualisation, while reshape2 contains functions to change the structure of data frames (reshape them). Let’s load the libraries using library(). library(rafalib) library(reshape2) Now we can use the function mypar() from “rafalib” to tell R to create a grid with 6 spaces. Then, we use apply() to plot histograms of each of the columns in our data frame data2 (element B in the “database” list”). We will discuss extensively what a histogram is in our next session, but for now let’s just think about it as a way to “see” our data. mypar(3,2) apply(X=database$B[,2:dim(database$B)[2]], MARGIN=2, FUN=hist, main=&quot;&quot;,xlab=&quot;&quot;) ## $heights ## $breaks ## [1] 125 130 135 140 145 150 155 160 165 170 175 180 185 190 195 200 205 ## ## $counts ## [1] 3 10 60 152 434 932 1466 1942 1956 1504 860 442 180 50 5 4 ## ## $density ## [1] 0.00006 0.00020 0.00120 0.00304 0.00868 0.01864 0.02932 0.03884 0.03912 0.03008 0.01720 0.00884 0.00360 0.00100 0.00010 0.00008 ## ## $mids ## [1] 127.5 132.5 137.5 142.5 147.5 152.5 157.5 162.5 167.5 172.5 177.5 182.5 187.5 192.5 197.5 202.5 ## ## $xname ## [1] &quot;newX[, i]&quot; ## ## $equidist ## [1] TRUE ## ## attr(,&quot;class&quot;) ## [1] &quot;histogram&quot; ## ## $BMIs ## $breaks ## [1] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ## ## $counts ## [1] 4 2 36 151 395 926 1475 1988 1925 1497 956 400 189 45 10 1 ## ## $density ## [1] 0.0004 0.0002 0.0036 0.0151 0.0395 0.0926 0.1475 0.1988 0.1925 0.1497 0.0956 0.0400 0.0189 0.0045 0.0010 0.0001 ## ## $mids ## [1] 19.5 20.5 21.5 22.5 23.5 24.5 25.5 26.5 27.5 28.5 29.5 30.5 31.5 32.5 33.5 34.5 ## ## $xname ## [1] &quot;newX[, i]&quot; ## ## $equidist ## [1] TRUE ## ## attr(,&quot;class&quot;) ## [1] &quot;histogram&quot; ## ## $ages ## $breaks ## [1] -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 60 65 ## ## $counts ## [1] 1 17 46 198 445 998 1558 1932 1926 1429 845 390 157 49 6 3 ## ## $density ## [1] 0.00002 0.00034 0.00092 0.00396 0.00890 0.01996 0.03116 0.03864 0.03852 0.02858 0.01690 0.00780 0.00314 0.00098 0.00012 0.00006 ## ## $mids ## [1] -12.5 -7.5 -2.5 2.5 7.5 12.5 17.5 22.5 27.5 32.5 37.5 42.5 47.5 52.5 57.5 62.5 ## ## $xname ## [1] &quot;newX[, i]&quot; ## ## $equidist ## [1] TRUE ## ## attr(,&quot;class&quot;) ## [1] &quot;histogram&quot; ## ## $sex ## $breaks ## [1] 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 ## ## $counts ## [1] 5019 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4981 ## ## $density ## [1] 10.038 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 9.962 ## ## $mids ## [1] 0.025 0.075 0.125 0.175 0.225 0.275 0.325 0.375 0.425 0.475 0.525 0.575 0.625 0.675 0.725 0.775 0.825 0.875 0.925 0.975 ## ## $xname ## [1] &quot;newX[, i]&quot; ## ## $equidist ## [1] TRUE ## ## attr(,&quot;class&quot;) ## [1] &quot;histogram&quot; ## ## $sleep ## $breaks ## [1] -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## ## $counts ## [1] 3 8 58 167 419 927 1510 1945 1898 1580 882 402 147 40 10 4 ## ## $density ## [1] 0.0003 0.0008 0.0058 0.0167 0.0419 0.0927 0.1510 0.1945 0.1898 0.1580 0.0882 0.0402 0.0147 0.0040 0.0010 0.0004 ## ## $mids ## [1] -0.5 0.5 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5 10.5 11.5 12.5 13.5 14.5 ## ## $xname ## [1] &quot;newX[, i]&quot; ## ## $equidist ## [1] TRUE ## ## attr(,&quot;class&quot;) ## [1] &quot;histogram&quot; The function melt() from reshape2 combines all the different columns of a data frame into a single columns and adds an extra column of labels. This way of restructuring the data is useful for easier visulisation or manipulation. dat ## names heights BMIs age sleep ## 1 Jimmy 183.1423 28.16605 21 8.424872 ## 2 Amanda 165.0718 24.00776 23 7.344862 ## 3 Glenn 148.0559 24.59611 23 8.067157 ## 4 Toby 180.7543 26.73520 40 7.626297 ## 5 Ren 162.6167 27.78964 19 6.636333 ## 6 Amanda 168.0388 28.41036 35 5.209633 melt(dat) ## Using names as id variables ## names variable value ## 1 Jimmy heights 183.142281 ## 2 Amanda heights 165.071790 ## 3 Glenn heights 148.055908 ## 4 Toby heights 180.754261 ## 5 Ren heights 162.616653 ## 6 Amanda heights 168.038833 ## 7 Jimmy BMIs 28.166054 ## 8 Amanda BMIs 24.007762 ## 9 Glenn BMIs 24.596110 ## 10 Toby BMIs 26.735198 ## 11 Ren BMIs 27.789636 ## 12 Amanda BMIs 28.410357 ## 13 Jimmy age 21.000000 ## 14 Amanda age 23.000000 ## 15 Glenn age 23.000000 ## 16 Toby age 40.000000 ## 17 Ren age 19.000000 ## 18 Amanda age 35.000000 ## 19 Jimmy sleep 8.424872 ## 20 Amanda sleep 7.344862 ## 21 Glenn sleep 8.067157 ## 22 Toby sleep 7.626297 ## 23 Ren sleep 6.636333 ## 24 Amanda sleep 5.209633 1.7 Writing data from R Once we’ve finished manipulating data, we might want to permanently store it in our computer (hard drive) and not just in RAM. R allows us to easily store data as files. There are two ways of writing data from R: 1.7.1 Writing human readable files We might want to write the data in a format that is human readale. This means, a format that any text editor can open and which contains ASCII characters. This can be done with the “write.table()” function. For example, we can store the data frame dat2 into a text file as follows: write.table(dat2, file=&quot;~/Desktop/Big_data_summer_course_2018/Data/example_data.txt&quot;) However, write.table() will automatically add quotes to every value in the text file. You can check this yourself opening the output file. To avoid this, simply set the argument quote to FALSE: write.table(dat2, file=&quot;~/Desktop/Big_data_summer_course_2018/Data/example_data_noQuotes.txt&quot;, quote=FALSE) By default, write.table() separates the values with spaces. Sometimes we might want to separate them using tab (“). For example, genomic distances and coordinates are commonly saved as tabulated files. To do this, specify the separator character by setting sep to”. write.table(dat2, file=&quot;~/Desktop/Big_data_summer_course_2018/Data/example_data.tab&quot;, quote=FALSE, sep=&quot;\\t&quot;) Finally, one of the most common data file formats is the comma separated value (csv), which can be read by any spreadsheet software (eg. Excel). To create this type of files, we can set sep=“,” or simply use the function “write.csv()”. write.csv(dat2, file=&quot;~/Desktop/Big_data_summer_course_2018/Data/example_data.csv&quot;, quote=FALSE) 1.7.2 Writing binary files Often we will be interested in saving objects which were difficult to generate but are too complicated to be stored in a human readable format. For instance, if we wanted to save our object “database”, which is a list containing two data frames, we could not use write.table. In these cases, an alternative is to save the data as a binary file (R data file). This is easily done with the function saveRDS(). It is recommended (though not absolutely necessary) to save the files with the suffix rds. saveRDS(database, file=&quot;./Data/example_database.rds&quot;) saveRDS() is standardised so that R will always save data objects in the same binary format regardless of computer architecture. This means that an RDS object generated in one computer can be taken to any other computer and read into R. 1.8 Reading data into R All the data types in the previous section can be read into R. This is done with the following function: To read from a csv, we use read.csv() mydata &lt;- read.csv(&quot;./Data/example_data.csv&quot;) head(mydata) ## X IDs heights BMIs ages sex sleep ## 1 1 1 150.0048 22.62468 40 0 6.109269 ## 2 2 2 164.2885 29.59051 10 0 4.998648 ## 3 3 3 172.1462 26.37399 39 1 6.978927 ## 4 4 4 172.8576 24.92409 11 0 7.996945 ## 5 5 5 174.9708 29.12725 16 0 10.933217 ## 6 6 6 166.7890 27.13721 19 0 5.266457 Note that read.csv() automatically recognises the header. If this is not what you want, you can set the header arugment to FALSE. Also, read.csv() assumes there are no row names. If we want the first column (or any other column) to be used as row names, we just need to set row.names to be the column number. In this case, row.names = 1 will use the IDs as row names: mydata &lt;- read.csv(&quot;./Data/example_data.csv&quot;, row.names = 1) head(mydata) ## IDs heights BMIs ages sex sleep ## 1 1 150.0048 22.62468 40 0 6.109269 ## 2 2 164.2885 29.59051 10 0 4.998648 ## 3 3 172.1462 26.37399 39 1 6.978927 ## 4 4 172.8576 24.92409 11 0 7.996945 ## 5 5 174.9708 29.12725 16 0 10.933217 ## 6 6 166.7890 27.13721 19 0 5.266457 To read from a space or tab separated files, we use read.table() as follows: mydata &lt;- read.table(&quot;./Data/example_data.tab&quot;, row.names = 1) head(mydata) ## IDs heights BMIs ages sex sleep ## 1 1 150.0048 22.62468 40 0 6.109269 ## 2 2 164.2885 29.59051 10 0 4.998648 ## 3 3 172.1462 26.37399 39 1 6.978927 ## 4 4 172.8576 24.92409 11 0 7.996945 ## 5 5 174.9708 29.12725 16 0 10.933217 ## 6 6 166.7890 27.13721 19 0 5.266457 Finally, we can read a binary (R data) file using readRDS(). Note that the list has exactly the same structure as it had before, and that you can easily access both of the data frames stored in it. mydatabase &lt;- readRDS(&quot;./Data/example_database.rds&quot;) lapply(mydatabase,head) ## $A ## names heights BMIs age sleep ## 1 Jimmy 173.9124 26.88027 21 7.571300 ## 2 Amanda 168.9660 24.66474 23 6.059818 ## 3 Glenn 180.5230 28.09904 23 3.385440 ## 4 Toby 162.3769 26.63590 40 5.273129 ## 5 Ren 167.9762 27.91876 19 8.569787 ## 6 Amanda 157.6978 24.85252 35 4.911579 ## ## $B ## IDs heights BMIs ages sex sleep ## 1 1 150.0048 22.62468 40 0 6.109269 ## 2 2 164.2885 29.59051 10 0 4.998648 ## 3 3 172.1462 26.37399 39 1 6.978927 ## 4 4 172.8576 24.92409 11 0 7.996945 ## 5 5 174.9708 29.12725 16 0 10.933217 ## 6 6 166.7890 27.13721 19 0 5.266457 1.9 Project work Now let’s try to apply what you just learn to your project data! We will begin by trying to solve the following tasks: Open a new R session and create a new R file. You can use this file as a place for experimentation Try reading your data into R. Most likely this data will be in the form of a CSV file or a table (text file) How many variables does your data contain? Which is the class of each of theses variables? Can you tabulate these variables? Can you identify which of them are quantitative and which qualitative? Try taking a sample of 10 elements from each of these variables "],["univariate-data-analysis.html", "Chapter 2 Univariate Data Analysis 2.1 Visualising univariate data 2.2 Summarising univariate data 2.3 Comparing data to probability distributions 2.4 Box plots 2.5 Data as a random sample 2.6 Exercises 2.7 References", " Chapter 2 Univariate Data Analysis In order to study big data, we first have to learn how to analyse the simplest type of information: univariate data. The word “univarite” refers to a set of information which contains only one variable. For example, a school database with grades for all students can be seen as univariate, because there is only one variable of interest: grades. Two or more variables can be put together to form bivariate or multivariate data. This will be the topic of future sessions. In this example code, we will be working with the results of a study done in the 1970s. In this study, researchers assessed the levels of pollution accross multiple cities in the US by measuring the average annual temperature, number of manufacturing enterprises, population size, wind speed, annual precipitation and levels of the pollutant sulfur dioxide (SO2). Because it contains all of these pieces of information, this data set is multivariate. However, for now we will only focus on one of those variables: the average annual temperature. Let’s start by loading the example data set. To access it you will have to install the R package “HSAUR2”. You can do this by running the following line, remember to make sure you are connected to the internet. install.packages(&quot;HSAUR2&quot;) Now that the installation has completed, you can load the package with the following line. library(HSAUR2) We are ready to start. Loading the HSAUR2 package will add a series of variables to R. One of those is “USairpollution”, which contains our example data. Let’s assign this data frame to the variable “dat” (dat standing for data). dat &lt;- USairpollution You can use head() to look at the first 6 lines of the table. Note how each of the rows is named after a US city, and each column contains a different piece of information for each city: sulfur dioxide levels, number of manufacturing enterpreises, population size, etc… head(dat) ## SO2 temp manu popul wind precip predays ## Albany 46 47.6 44 116 8.8 33.36 135 ## Albuquerque 11 56.8 46 244 8.9 7.77 58 ## Atlanta 24 61.5 368 497 9.1 48.34 115 ## Baltimore 47 55.0 625 905 9.6 41.31 111 ## Buffalo 11 47.1 391 463 12.4 36.11 166 ## Charleston 31 55.2 35 71 6.5 40.75 148 If you want to know how many cities and how many different variables are in this table, use the function dim() as follows. dim(dat) ## [1] 41 7 Our table contains 7 variables for 41 different US cities. Let’s extract only the “temperature” variable and assign it to a new variable called temps (temps for temperatures). temps &lt;- dat$temp 2.1 Visualising univariate data We must always look at the data we have before doing any analysis with it. This is because sometimes data sets contain errors, outliers or missing values, and visually inspecting them reveals all of these issues Furthermore, some patterns can be very clear to the human eye and will suggest us what we have to do next. There are several methods for representing univariate data visually. Let’s look at a few of them. 2.1.1 Dot plot / Strip chart Each of the cities has its own particular temperature. How to get an idea of how hot or cold they are? The easiest way to do this is by ordering the cities from coldest to hottest and then adding them one after another in a line. This type of graph is called a “dot plot” (because each number is usually represented by a dot) or “strip chart”. To create a strip chart in R, simply use the stripchart() function, specifying the name of your data frame: stripchart(temps) This is a good start, but the plot has no title and the axes have no names. This is really bad practice! We can add titles via the argument “main” and label the X and Y axes using the arguments “xlab” and “ylab”. Let’s also change the squares into dots, which are more commonly used. To do that let’s set the argument “pch” to 19. “pch” stands for “Plotting Character”, and tells R which figure to use for each of the elements in our graph. Each number correspond to a different figure and you can use anything from triangles and rectangles to crosses and dots. Let’s also add “stack” in the method argument. This tells R that, if two cities have very similar temperatures, they should be stacked one on top of the other. In this way we know that multiple cities fall in the same range. stripchart(temps, method = &quot;stack&quot;, pch=19, main=&quot;Temperature of US cities&quot;, xlab=&quot;Temperature (ºF)&quot;) 2.1.2 Histograms But sometimes having dots ordered by value is not enough, because it doesn’t tell us anything quantitative about the data. It is more common to use histograms. To create a histogram, we divide the range of temperatures in equal intervals (sometimes called “bins”) and count how many cities fall in each bin. We then create bars, and the height of each bar reflects the number of cities with temperature in that range. To create a histogram in R simply use the funciton hist() hist(temps, main=&quot;Temperature of US cities&quot;, xlab=&quot;Temperature (ºF)&quot;) However, sometimes even this is not accurate enough. For example, now we know there are 13 cities with temperature between 50 and 55 ºF, but how big actually is this number? If we only had information for 15 cities it would be huge, but if we had information for 100 it would be pretty small. For this reason, sometimes it is better to use “percentage (proportion) of cities” and not number of cities. To do this in R, set the frequency argument (“freq”) to FALSE. hist(temps, freq = FALSE, main=&quot;Temperature of US cities&quot;, xlab=&quot;Temperature (ºF)&quot;) Note how the y axis has changed! Now we know that 7% of the cities we have information for have a temperature of 50 to 55 ºF. You can also increase the numebr of bars (or, to be precise, the number of bins) using the “breaks” argument. For example, setting breaks=20 will cause R to plot approximately 20 bars in the histogram. hist(temps, freq=FALSE, main=&quot;Temperature of US cities&quot;, xlab=&quot;Temperature (ºF)&quot;, breaks=20, col=&quot;grey&quot;) This allows us to have more detailed information on the data (we call this “granularity”). For example, we can now see that there are 4 or 5 cities with unusually high temperatures. These might be outliers. Notice how we also changed the colour of the bars to grey using the argument “col” (col for color). 2.1.3 Density plot Sometimes drawing bars might not be the best option. We can create an approximate “line” which represents the same information using the function density(). Density() does more than simply creating a line! It is actually creating tiny “sliding bins” and estimating the value for each of them, then joining the values by lines. The way these values are estimated is by using “kernels”. Dernels and densities are extremely important when dealing with multivariate data. A plot created in such a way is called a “density plot”. To create one in R, simply use density() followed by plot() as follows: plot(density(temps), main=&quot;Temperature of US cities&quot;, xlab=&quot;Temperature (ºF)&quot;) This density plot reveals, again, a few cities which are unusually hot. Let’s change the color of the density plot to blue. plot(density(temps), main=&quot;Temperature of US cities&quot;, xlab=&quot;Temperature (ºF)&quot;, col=&quot;blue&quot;) Now that we’ve seen the temperature data with our own eyes, let’s look at more formal ways to get useful information out of it. 2.2 Summarising univariate data Perhaps the most question to ask with a data set like this one is the following: “What is the most common temperature?” or, in other words, “How hot or cold are MOST cities?” 2.2.1 Mean To answer this we can calculate a simple average. The way to do this in R is via the mean() function. mean(temps) ## [1] 55.76341 This tells us most US cities have a temperature close to 56ºF. Let’s see how this agrees with out visual inspection of the data. We can create a density plot and add a vertical line indicating the mean. To add vertical (or horizontal) lines, we use the function abline(), the v arguments tells R we want a vertical line. plot(density(temps), main=&quot;Temperature of US cities&quot;, xlab=&quot;Temperature (ºF)&quot;) abline(v = mean(temps)) The mean temperature is indeed very close to the interval where most cities fall. However, it does not align with the most common value (the highest point of our density plot). Why is this? One possibility is that the unusually hot cities are causing this mean to be too high. We call this “skewing” the mean. 2.2.2 Median Another way of answering this same question is by ordering all the cities from coldest to hottest and finding the one right in the middle of the series. What is the temperature of this “middle” city? We call this value the median. To calculate medians in R simply use the funciton median(). median(temps) ## [1] 54.6 Let’s now add both the mean and the median to the previous density plot. The mean will be a red line and the median a blue one. plot(density(temps), main=&quot;Temperature of US cities&quot;, xlab=&quot;Temperature (ºF)&quot;) abline(v = mean(temps), col=&quot;red&quot;) abline(v = median(temps), col=&quot;blue&quot;) The median is closer to the position where, according to our eyes, the most common temeprature should be. This is because the median, as opposed to the mean, is “robust”. This means that even if we have outliers (cities with very high or low temperatures), the median will remain almost the same. Let’s add the same lines to our first strip chart. stripchart(temps, main=&quot;Temperature of US cities&quot;, xlab=&quot;Temperature (ºF)&quot;, method = &quot;stack&quot;, pch=19) abline(v = mean(temps), col=&quot;red&quot;) abline(v = median(temps), col=&quot;blue&quot;) 2.2.3 Quantiles Sometimes we not only want to know what is the most common temperature, but also how many cities are cold, how many hot and how many not so cold but not so hot. To answer this question we could simply order the cities from coldest to hotest and then divide them in four groups: the first fourth (25%) are the coldest, the next fourth are the “kind of cold”” ones, next are the “kind of warm” cities, and the final fourth are the hotest. Then we can ask: what are the cutting points between these four groups? After which temperature do we no longer call a city cold but warm? We call these “cutting points” the quantiles of the data. To create these 5 quantiles, which divide the data in fourths, you can use the function quantile(). quantile(temps) ## 0% 25% 50% 75% 100% ## 43.5 50.6 54.6 59.3 75.5 This tells us that the 25% coldest cities have temperatures below 43 ºF. If this sounds too abstract, then let’s visualise it. Let’s go back to our original strip chart and add 4 vertical lines, one for each quantile. stripchart(temps, main=&quot;Temperature of US cities&quot;, xlab=&quot;Temperature (ºF)&quot;, method = &quot;stack&quot;, pch=19) abline(v=quantile(temps)) Notice how these quantiles divide the data in fourths. Because of this, in statistics they have a special name: quartiles. Another way to obtain the quartiles of a data set is by using the function summary(). summary(temps) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 43.50 50.60 54.60 55.76 59.30 75.50 Notice how the middle quantile is the same as the median. But we don’t always have to divide our data in fourths. We can, for instance, divide it in tenths. To do this, we use the quantile() function and add a new argument called “probs” (probs for probabilities). We will set probs to be the series of numbers 0.1, 0.2, 0.3, etc… until we reach 1. This tells R we want the first 0-10% of the data, then the 10-20% and so on. plot(density(temps), main=&quot;Temperature of US cities&quot;, xlab=&quot;Temperature (ºF)&quot;) abline(v=quantile(temps, probs = seq(0,1,0.1))) We call these specific quantiles the “deciles”. Using the same procedure, we can divide the data in 100 tiny bins. We call the cutting points for this partition “percentiles”. Each of the bins created by this process will contain 1% of the data. plot(density(temps), main=&quot;Temperature of US cities&quot;, xlab=&quot;Temperature (ºF)&quot;) abline(v=quantile(temps, probs = seq(0,1,0.01))) However, notice that this is purely thoeretical. We do not actually have information for 100 cities, only for 41! R is thus estimating how these quantiles would be if we indeed had 100 cities or more. This becomes very obvious in our strip chart: stripchart(temps, main=&quot;Temperature of US cities&quot;, xlab=&quot;Temperature (ºF)&quot;, method = &quot;stack&quot;, pch=19) abline(v=quantile(temps, probs = seq(0,1,0.01))) Notice how some of the bins do not contain any actual data points: if we increased our data size to 100 or 1000 cities we would expect these areas to start being filled up. 2.2.4 Standard deviation Another question we can ask from this data is: “How different between each other are US cities in terms of temperature?”. We can think of this question as: “How different is the temperature of each US city to the average (mean) temperature?” This might seem like an abstract question, but we can visualise it. Let’s represent the temperature of each city as a dot, and the average as a vertical line (just like in our strip chart). How far is each dot from the line? This is represented here by dashed line segments. The question we are asking can be thought of as “how long are these lines usually?” plot(x = temps, y=1:41, pch=19, main=&quot;Temperature of US cities&quot;, xlab=&quot;Temperature (ºF)&quot;, ylab=&quot;&quot;, yaxt = &quot;n&quot;) abline(v=mean(temps)) segments(x0 = temps, x1 = mean(temps),y0=1:41,y1=1:41, lty=&quot;dotted&quot;) To find these values, let’s substract the mean temperature from the temperature of each single city. We will assign this value to the variable “diffs”. diffs &lt;- temps - mean(temps) Then, we can simply ask: how big are these differences on average? Let’s apply the mean function to them. mean(diffs) ## [1] -2.166289e-15 We see that the number is extremely small! However, we can see in the plot above that this is not true: cities are sometimes very far from the average line. So what went wrong? If we look at the content of diffs we will see the problem right away: some of this differences are negative and some are possitive, and you cannot simply add positives and negatives. We are not interested in the difference being positive or negative (colder or hotter cities), we are just interested in how big it is. diffs ## [1] -8.1634146 1.0365854 5.7365854 -0.7634146 -8.6634146 -0.5634146 -5.1634146 -1.7634146 -6.0634146 -4.2634146 10.4365854 -3.8634146 -6.7634146 -5.8634146 ## [15] -6.6634146 13.1365854 -3.4634146 12.6365854 -1.2634146 5.2365854 -0.1634146 5.8365854 19.7365854 -10.0634146 -12.2634146 3.6365854 12.5365854 3.5365854 ## [29] -4.2634146 -1.1634146 14.5365854 -5.3634146 -5.7634146 2.0365854 -4.7634146 0.9365854 -4.6634146 0.1365854 1.5365854 0.8365854 -1.7634146 To solve this problem, we can square each of this numbers. This, because squares are always possitive, regardless of the sign of the original number. for instance 2^2 and (-2)^2 are both 4. Let’s do this and see if we can get rid of the negative sign. diffs^2 ## [1] 66.64133849 1.07450922 32.90841166 0.58280190 75.05475312 0.31743605 26.66085068 3.10963117 36.76499703 18.17670434 108.92231410 14.92597264 45.74377751 ## [14] 34.37963117 44.40109459 172.56987507 11.99524093 159.68328971 1.59621654 27.42182629 0.02670434 34.06572873 389.53280190 101.27231410 150.39133849 13.22475312 ## [27] 157.16597264 12.50743605 18.17670434 1.35353361 211.31231410 28.76621654 33.21694825 4.14767995 22.69011898 0.87719215 21.74743605 0.01865556 2.36109459 ## [40] 0.69987507 3.10963117 It seems to have worked. Now we can calcualte the average distance using mean. However, remember we applied a square, so now we have to revert it. We revert it using the opposite operation: a square root. sqrt(mean(diffs^2)) ## [1] 7.139029 This value (the average difference between any particular temperature and the average temperature) is called “standard deviation”. It is a measure of the “spread” of the data, which in common words means how different cities are to each other. We don’t have to go into the trouble of calculating all of these squares, means, and squared roots every time we want the standard deviation. R already has a function that does that for us called sd(). Let’s apply it to the data: sd(temps) ## [1] 7.227716 Notice how it is almost the same number. The difference in decimal points is simply because of precision. 2.3 Comparing data to probability distributions We might also be interested in how our observations are “distributed”. You can think of this as “what kind of shape does the histogram and density plots have?” Can we see one very frequent value, forming a bell shape? Or do we see two or three frequent values? If we see a bell shape, is it symmetric? Or is the right or left tail longer? But finding the distribution that describes a dataset is much more than simply knowing its “shape”: it is a deep statistical question that can help us understand how the data behaves. During the theoretical session that accompanies this exercise you will learn more about statistical distributions. Multiple of these exist such as the binomial, the Poisson, the gamma, the T (Student’s) and the normal distribution. For now, let’s focus only on the normal distribution. We say that data is normally distributed if it has a symmetric “bell shape” (the proper name for this shape is ‘Gaussian’, because Gauss was the mathematician who described it) with one very frequent value. We only need two numbers to define a normal distribution: the mean the standard deviation R allows us to very easily create random numbers from a normal distribution. For example, let’s use the mean and standard deviation we calculated before as the parameters of a normal distribution. Using them, let’s “simulate” 1000 data points. You can think of this as simulating in R data for 1000 US cities. Of course these cities do not exist, but in the computer they will behave as “the average US city”. To generate random numbers using the normal distribution in R, use the function rnorm() and specify how many random numbers you need, what is the mean and what is the standard deviation. We will assign these thousand random numbers to the variable “cities”. cities &lt;- rnorm(n = 1000, mean = mean(temps), sd = sd(temps)) Now, let’s create a histogram of such simulated temperatures. hist(cities, freq=FALSE, breaks=50, col=&quot;grey&quot;, main=&quot;Simulated temperatures&quot;, xlab=&quot;Temperature (ºF)&quot;) We can see that they indeed form a symmetric bell shape around our mean. Now we are able to ask: “Is our real data on temperature of US cities normally distributed?” To answer this question, let’s put our simulated (theoretical) values and our real values together. Do they have the same properties? Do we see the same shape? We will plot the normal distribution in black and the real data in blue plot(density(temps), main=&quot;Simulated vs real temperatures&quot;,xlab=&quot;Temperature (ºF)&quot;, col=&quot;blue&quot;) lines(density(cities)) You can already see that our real data, even though similar to the normal distribution, does not really have the same shape. There are a number of really hot cities that appear like a bump towards the right side of the plot. But evaluating this with our eyes is not rigorous enough! To make this a more formal test let’s do the following: We create percentiles for our data, as we did above: plot(density(temps), main=&quot;Temperature of US cities&quot;, xlab=&quot;Temperature (ºF)&quot;) abline(v=quantile(temps, probs = seq(0,1,0.01))) We do the same with the simulated (theoretical) data: plot(density(cities), main=&quot;Simulated temperatures&quot;, xlab=&quot;Temperature (ºF)&quot;) abline(v=quantile(temps, probs = seq(0,1,0.01))) Now all we have to do is compare both sets of quantiles. Are they the same? We can do this by plotting the theoretical against the real quantiles. If they are the same, they should follow a straight line. We call this a quantile-quantile plot, sometimes called a “QQ-plot”. There is a very easy way to do this in R. You can use the function qqnorm(), which will automatically create such a plot. You can even add a straight line to test if the points fall on it using qqline(). Note, however, that this function only works for normal distributions. qqnorm(temps, main=&quot;Temperature of US cities&quot;) qqline(dat$tem) Now we can see that, indeed, there are some outlier cities at the very top. However, the rest o the data does follow the line, so we can confidently say they are normally distributed! 2.4 Box plots Finally, let’s study one last type of graph: the box plot. The box plot is a very simple plot which summarises all of our data. It is a box which contains the central 50% of the data. This means that the limits of the box are the 1st and 3rd quartile. We also add a line inside the box representing the median and whiskers that extend to the upper and lower limits of the data. If there are any obvious outliers, they are plotted as tiny dots outside the plot. To create a boxplot in R, you can ue the boxplot() function as follows: boxplot(temps, main=&quot;Temperature of US cities&quot;, ylab=&quot;Temperature (ºF)&quot;) 2.5 Data as a random sample As discussed in the theoretical session, statistics consider any data set as a random sample. This means that, of all possible US cities at all possible times, we took a sample of 41 of them. But if we repeated this experiment again, we would get a different set of 41 temperatures. In fact, if we repeated it 100 times we would get 100 different sets of 41 values! Let’s simulate this in R. We have already created a “fictional” set of 1000 cities above. Let’s take a sample of 41 temperatures out of thsoe 1000 and calculate the mean. Let’s then repeat this 10 times. We will thus get 10 means. for(i in 1:10){ print(mean(sample(x = cities, size = 41))) } ## [1] 56.00563 ## [1] 56.33187 ## [1] 55.07881 ## [1] 56.11799 ## [1] 54.50222 ## [1] 54.5842 ## [1] 56.77567 ## [1] 56.05676 ## [1] 57.92634 ## [1] 53.68575 You can see that every mean is different to the other, but at the same time they are all kind of similar, and they are also similar to the mean in our real data (54.6). Let’s now repeat this 1000 times and create a histogram! averages &lt;- c() for(i in 1:1000){ averages[i] &lt;- (mean(sample(x = cities, size = 41))) } hist(averages, breaks = 20, col=&quot;grey&quot;, main=&quot;&quot;, xlab=&quot;Average&quot;) abline(v=c(quantile(averages, probs = c(0.025,0.975)))) Notice that I’ve added two vertical lines: 95% of the times our average (mean) is between these two lines. We call this a confidence interval. In this case, a 95% confidence interval. What this means is that, if the 1000 cities we simulated were real and if we repeatedly measured their temperatures, 95% of the times we would get an average between 53.16 and 57.45 ºF. quantile(averages, probs = c(0.025,0.975)) ## 2.5% 97.5% ## 53.63041 58.09983 So from now on, every time you calculate an average, median or standard deviation remember that the value you get is only one of hundreds of possibilities. If you repeat the experiment you will get something different! We call this a “random variable”. Means and standard deviations are always random variables, and thus it is always a good idea to not only say their value, but also add a confidence interval. 2.6 Exercises Now you will have some time to analyse your own project data. You can use any of the functions we learnt here by simply copying and pasting them in a new R file. You can start by askig the following questions: How many numeric variables are in my data? Focus on only one of them Create a histogram of such a variable Create a density plot Create a box plot What is the average of this variable? Calculate the mean and the median How spread is this variable? Calculate the standard deviation Is this data normally distributed? Create a qqplot What does this tell you about the topic of your data? Can you learn something from it? 2.7 References Dalgaard P. (2008). Introductory statistics with R. London: Springer. Sokal RR and Rohlf FJ. (1981), Biometry. San Francisco: W. H. Freeman (2nd edition). Everitt B and Holthorn T. (2011). An Introduction to applied multivariate analysis with R. London: Springer. "],["bivariate-data-analysis.html", "Chapter 3 Bivariate Data Analysis 3.1 Visualising two variables 3.2 Applying transformations to a variable 3.3 Using ggplot2 to create personalised plots 3.4 Exercises 3.5 Covariance 3.6 Correlation 3.7 Predictions and linear models 3.8 Correlation does not equal causation 3.9 Exercises", " Chapter 3 Bivariate Data Analysis During the previous session we learned how to analyse data with a single variable. Most of the times, however, big data will not contain one but multiple variables (hence its size!). To move forward in this direction, let’s now learn how to analyse datasets which contain two variables. We call this type of data bivariate. Before starting this session you will need to install the packages “MVA”, “dslabs” and “ggplot”. You can do that by running the following lines: install.packages(&quot;MVA&quot;) install.packages(&quot;dslabs&quot;) install.packages(&quot;ggplot2&quot;) install.packages(&quot;ggrepel&quot;) Now that the installation has completed, let’s load them. library(HSAUR2) library(MVA) library(dslabs) library(ggplot2) library(ggrepel) library(rafalib) Now we are ready to start. Loading the package “dslabs” will add a number of datasets to your R session. One of them is “murders”, which contains FBI records for murders by gun in several US states. These records were collected in 2010. Let’s assign this data set to a variable called “dat” (dat for data). dat &lt;- murders Let’s look at the first few lines of the data set. head(dat) ## state abb region population total ## 1 Alabama AL South 4779736 135 ## 2 Alaska AK West 710231 19 ## 3 Arizona AZ West 6392017 232 ## 4 Arkansas AR South 2915918 93 ## 5 California CA West 37253956 1257 ## 6 Colorado CO West 5029196 65 Notice that the data contains four variables: Name of US state, abreviation of the state, region to which it belongs, size of the population for that state and total number of murders by gun registered. Of these variables, we will focus on the two numeric ones: population size and number of gun murders. Throughout this session we will try to ask a very simple question: Are this two variables related? Or, in other words, are there more murders by gun in places where the population is bigger? 3.1 Visualising two variables Eery one of these two variables (poulation size and number of murders) can be analysed using the techniques we learnt about in the previous session. For example, we can create a histogram of population size, and find out how populated states tend to be: hist(dat$total, breaks=10, main=&quot;&quot;, xlab=&quot;Population&quot;, freq=F, col=&quot;grey&quot;) We can explore the number of murders in the exact same way: hist(dat$total, breaks=10, main=&quot;&quot;, xlab=&quot;Population&quot;, freq=F, col=&quot;grey&quot;) This already tells us that US states are highly variable both in terms of population and number of murders. In other words, while some states (like California) have more than 30 million inhabitants, some others (as Wyoming) hace only half a million. However, we are analysing the variables in isolation. In other words, we are analysing them separately instead of together (jointly). Let’s now learn about some methods to visualise them at the same time, perhaps this can inform us of any relationship between them. 3.1.1 Scatter plot A scatter plot is a plot of two variables in which each variable is encoded in each of the axes (X axis and Y axis). Pairs of valus are represented by points with an X and a Y coordinate. It is by far the most used plot in the entire field of statistics (and perhaps in a lot of sciences). Let’s create a plot with population size in the X axis and number of murders on the Y. To do this, we use the function “plot()”. The first value is the x and the second value the y. plot(dat$population, dat$total, xlab=&quot;Population size&quot;, ylab=&quot;Number of gun murders&quot;) If we want to preserve the information we got from the historgrams for each variable and on top of that plot them together, we can add “marginal” distributions. They are called marginal because they literally are drawn on the “margins” or axes. You can think of them as tiny stripcharts/dot plots. To add marginal stripcharts in R, use the function rug(). plot(dat$population, dat$total, xlab=&quot;Population size&quot;, ylab=&quot;Number of gun murders&quot;) rug(dat$population, side=1) rug(dat$total, side=2) 3.1.2 Bivariate box plots If we want to know where the majority of our data is, we can use an “extension” of hte box plot in 2 dimensions (we learnt about the box plot in the previous session). This extension is called “bivariate box plot” (because it displays two variables) and we can draw it in R using the function “bvbox()”. bvbox(dat[,c(&quot;population&quot;,&quot;total&quot;)],xlab = &quot;Population in millions&quot;,ylab=&quot;Number of murders&quot;) We can read this plot is as follows: there is a central ellipse which is the equivalent to the box of a boxplot, the central 50% of the data will be inside it. Surrounding it, there is a bigger ellipse (dotted lines), this is the equivalent of the “whiskers” of the boxplot and most of the data (ideally all) should fall inside it. The potential outliers can are dots falling outside the ellipses. In this case, we can see 4 states with unusually high population and number of murders. These points are outisde the ellipses, so perhaps they are outliers. 3.2 Applying transformations to a variable But are those points in the precious plots really outliers? There is a slight problem with this: we saw before that our data is quite variable and spread. Some states have tens of millions of inhabitants and some others fewer than a million. It is difficult to compare states when they are so different! If you are still not convinced, look at the following density plot: plot(density(dat$population), main=&quot;&quot;, xlab=&quot;Population size&quot;) Do you see the long “tail” towards the left side of the plot? It is made of states with unusually high population size. In cases like this, when the numbers vary so much and can be tiny or massive, we sometimes need to “transform” the data. What this means is that we modify it in a way that makes it more compressed, comparable and easy to handle. One of the most common ways of “transforming” data is applying a logarithm. If you have studeid any logarithms before, you’ll remember that they are the opposite operation to exponents. Logarithms are very useful because they can be used to transform differences of hundreds or thousands into differences of units. For example, let’s look at the following series of numbers: 1, 10, 100, 1000, 100000, 1000000 They increase very fast! However, if we apply a simple logarithm (base 10) to them, the series becomes: 1, 2, 3, 4, 5, 6 Which is much easier to see and analyse. Let’s apply a logarithm to the population size: logpopulation &lt;- log10(dat$population+1) Note we had to add 1 before applying the logarithm. This is because log(0) equals infinity, and R cannot deal with inifinte numbers (and neither can we). Now let’s create a density plot of the “log-population size”: plot(density(logpopulation), main=&quot;&quot;,xlab=&quot;log10(Population size)&quot;) Compare this density plot with the previous one. Now the tail has dissapeared and our data looks like the usual “bell shape” we found in the previous session! The numbers on the x axis on this plot (5, 6, 7, 8) would be equivalent to 100000, 1000000, 10000000, 100000000 in the linear scale. Let’s apply the logarithm to the number of murders too and plot it “against” the population size (against here means in the same plot). Let’s also add marginal stripcharts logmurders &lt;- log10(dat$total + 1) plot(logpopulation,logmurders,xlab = &quot;log10( Population in millions )&quot;,ylab=&quot;log10( Number of murders )&quot;) rug(logpopulation, side=1) rug(logmurders, side=2) Now the plot looks much better! We can actually see all the points and they are evenly spaced. In fact, now it starts becoming obvious that there is some relationship between population size and number of gun murders. When one increses, the other increases too. They form some sort of straight line. Since the logarithms worked so well, let’s replace the original variables in our data with their transformations: dat$population &lt;- logpopulation dat$total &lt;- logmurders Let’s create a bivariate box plot now using the “transformed” (log) data. bvbox(dat[,c(&quot;population&quot;,&quot;total&quot;)], xlab = &quot;log( Population in millions )&quot;, ylab=&quot;log( Number of murders )&quot;) Notice how all the outliers have dissapeared. They were not really outliers, they were just too populated states compared with the rest and the comaprison was unfair. 3.3 Using ggplot2 to create personalised plots So far we have been using the functions plot(), hist(), density(), etc… However, there if we want to make very specific changes to our plot (like making our favorite point a different colour or changing the background) it becomes difficult. There is a better way to draw plots in R which allows us to modify every single aspect of the plot: it is done using the package ggplot. The ggplot package is big and learning how to use it takes time. We will not focus on this during this course, but let’s have a quick look at the things we can do with it. Let’s start by creating the same scatter plot we have above, but using ggplot: we use the funciton ggplot() and specify where our data is stored, next we use “aes()” and include inside the parenthesis the things we want R to use as X and Y variables, in this case the population and the number of murders. Then we add (+) an indication of the type of plot we want. In this case, we want a scatter plot where each value is a point, so we use “geom_point()” (which stands for geomtry of points). ggplot(data=dat, aes(x=population, y=total)) + geom_point() + xlab(&quot;log10(Population size)&quot;) + ylab(&quot;log10(Number of murders)&quot;) This looks nice, but the grey background is ugly. Let’s remove the grey! To do that, we simply add (+) another indication telling R to use a “white background (bw)”. You’ll start to notice how ggplot works: you use the + symbol to add more and more indications and specifications and make your plot personalised to what you really want. ggplot(data=dat, aes(x=population, y=total)) + geom_point() + xlab(&quot;log10(Population size)&quot;) + ylab(&quot;log10(Number of murders)&quot;) + theme_bw() Let’s now change the colout of the points to blue. To do this, we add a color indication (color=“blue”) inside geom_point(): ggplot(data=dat, aes(x=population, y=total)) + geom_point(color=&quot;blue&quot;) + xlab(&quot;log10(Population size)&quot;) + ylab(&quot;log10(Number of murders)&quot;) + theme_bw() But ggplot can do more than colouring points blue! In fact, we can use the colour not only as a visual decoration, but to actually represent something. For example, let’s tell R to color the points by region (each color will represent a US region). Notice how, whenever we refer to a variable inside our data set, we have to use aes(). ggplot(data=dat, aes(x=population, y=total)) + geom_point(aes(color=region)) + xlab(&quot;log10(Population size)&quot;) + ylab(&quot;log10(Number of murders)&quot;) + theme_bw() Now we can in fact read not only 2 but 2 variables in the same plots! This plot actually has three dimensions, one of which is the color. Have a go, can you find any particular region with higher population than the rest? If you have difficulty distinguishing colors, that is no problem. Instead of coding by color we can ask R to code by “shape”, so that each region is represented by a different figure (triangle, square, cross, etc…). It now looks like this: ggplot(data=dat, aes(x=population, y=total)) + geom_point(aes(shape=region), size=2) + xlab(&quot;log10(Population size)&quot;) + ylab(&quot;log10(Number of murders)&quot;) + theme_bw() We can even add a fourth variable to our plot! For example, let’s add labels with the abbreviated name of each state. In that way we will not only get their population and number of murders: we will also know which specific state we are talking about. To do this, we add yet another indication (+) called geom_text() and specify that we wan the label to be “abb”” the state abbreviation included in our data set. ggplot(data=dat, aes(x=population, y=total)) + geom_point(aes(color=region)) + xlab(&quot;log10(Population size)&quot;) + ylab(&quot;log10(Number of murders)&quot;) + geom_text(aes(label=abb)) + theme_bw() This is nice, however the labels no longer let us see the actual points or colours: they get on the way. To avoid this, instead of using geom_text we can use geom_text_repel, which tells R not to let the text crash with the points or with other text. Let’s see how it looks like now. ggplot(data=dat, aes(x=population, y=total)) + geom_point(aes(color=region)) + xlab(&quot;log10(Population size)&quot;) + ylab(&quot;log10(Number of murders)&quot;) + geom_text_repel(aes(label=abb)) + theme_bw() Much better and cleaner! Now we are ready to continue with more formal comparisons that rely not only on our eyes, but on mathematics and statistics. But before continuing, try applying some of this tools to your own project data. 3.4 Exercises Now you will have some time to analyse your own project data. You can use any of the functions we learnt here by simply copying and pasting them in a new R file. You can start by askig the following questions: How many numeric variables are in my data? Can any of them be related? Create a scatter plot two variables you suspect could be related. If you can, add marginal densities. Do these variables need any type of transformation? Or do they not? Are there any outliers? Create a bivariate box plot to find out Is there any evidence of relationship between the two variables? Create a new scatter plot, but now using the ggplot functions Try to add a 3rd variable to your plot by modifying the color or the shape Create a density plot 3.5 Covariance We previously learnt that, when analysing only one variable, we can calculate the standard deviation. The standard deviation is the “average distance” of observations to the mean. For example, in the following plots the average length of the dotted lines is the standard deviation. We can calculate the standard deviation for both population size and number of murders. But since we have two variables, now we can ask a new question: are they related? do they change in the same way? If one goes up, does the other one also go up? This question has a very simple meaning if we look at the plots below: it means, do both variables (population and murders) move “away” from the vertical line in the same direction? If gun murders moves to the right, does population also move to the right? To answer this question, we can simply calculate the length of those dotted lines for each variable, then multiply it. Finally, we find the average. The idea is the following: If both variables “move to the left” from the mean line, then both numbers will be negative and (-)(-) = + If both variables “move to the right”, then both numbers will be positive and (+)(+) = + If both variables “move in opposite directions:, then one will be positive and one negative, so (+)*(-) = - This means that: If we get a positive number, the variables are related. When one increases the other one increses too. We say that the variables “covariate”. If we get a negative number, the variables are also related, but inversely. When one increases the other one increses too. The variables also “covariate”. If we get zero, the variables are not related. We call this number covariance. The easiest way to calculate it in R is using the “cov()” function: cov(dat$population, dat$total) ## [1] 0.2613055 The number is positive, so number of murders and population size do covariate. Note, however, that because the covariance is a multiplication it will depend on how big your numbers are. This means it is sensitive to units: if we work with centimeters, kilometeres or meters, we will get three different numbers. Because of this, the actual value of the correlation does not tell us anything at all: the only thing we can inerpret is the sign. 3.6 Correlation It is dull that covariance will change depending on the units we use. After all, it should not matter in which units we measured our data! To solve this problem, instead of calculating the covariance you can calculate the correlation. Correlations are “dimentionless” (they have no units) and they are designed so that, regardless of the units your data is in, you always get the same number. Correlations are calculated in R very easily using the function “cor()”. corr &lt;- cor(dat$population, dat$total) corr ## [1] 0.8917199 Correlations always go from -1 to 1, and the number is very intuitive to interpret: If correlation is 1, the two variables have a perfect correlation: they form a line when plotted together If correlation is -1, the two variables have a perfect inverse correlation: they form a line with negative slope If correlation is 0, the variables are not related at all Anything in between are intermediate correlations, and the closer the number is to 1 (or -1) the stronger the relationship between variables is. For example, our data has a correlation of 0.89, which is very high. This tells us that population size and number of murders are quite related to each other: when one increases, the other one increases too. If possible, you should always add the corresponding correlation value to any scatter plot you create. This adds information on how strong the relation between the variables actually is not only to our eyes, but statistically. Let’s add this number to our previous plot. ggplot(data=dat, aes(x=population, y=total)) + geom_point(aes(color=region)) + xlab(&quot;log10(Population size)&quot;) + ylab(&quot;log10(Number of murders)&quot;) + theme_bw() + geom_text_repel(aes(label=abb)) + ggtitle(&quot;Gun murders in 2010. Cor = 8.892&quot;) 3.7 Predictions and linear models Let’s now look at the final and most interesting question: can we predict the average number of fun murders that will occur in a state if we know its population size? In statistics this type of problem is called “prediction”. Note, however, that we do not always want to “predict the future”. In this case, for instane, we want to know the value for a state with X number of inhabitants, but that state could’ve existed in the past, could exist in the present or could not even exist. So we will predicting does not always mean finding out things before they happen. In fact, a researchers spend a long time trying to predict past events in order to understand them better and find out what caused them. The variable we want to predict is our dependent variable (in this case number of murders) and the variables we basing our prediciton on are the independent variables or predictors (in this case the population size). Note that we are right now dealing with the simplest type of prediction: we only have one variable. More often, scientists use tens or hundreds of variables to predict. To do this they implement machine learning techniques. We will look at this in future sessions. The simplest type of predictive model is the “linear model”. A linear model assumes that you can represent the relationship between your two variables with a straight line (it has to be straight!). For example, like this: If our states came from an ideal world, they would all fall on top of the line, but since they come from a “real world” full of noise and random variation, they scatter around the line, surrounding it. The problem is, we do not know what this line looks like. Where does it start? Where does it end? How steep is it? It could be any of the following three lines, or any other line as a matter o fact: There are an infinite number of possibilities! How to find the appropriate one? As you will learn in the theoretical session accompanying this tutorial, the way find the ideal line (which we call the “best fit”) is by calculating the distance from every point to an imaginary line (we can change this imaginary line at our will) and adding up all these distances into a total number called the “Sum of squared residuals”. This name comes from the term “residual”, which is the statistical way of saying “distance from the point to the line” and from the fact that we square these numbers to eliminate all negative numbers. Then we find the line that minimises this number: the line that makes it as small as possible. What this means is we will find the line to which all points are closest to. We do this by applying differential calculus, which can be used to find “minimums” of functions. This might sound overly complicated, but it is ridiculously easy to do it in R. All you need to do is run the function lm() (“lm” stands for linear model) and tell R where your data is stored. You will also need to add a formula with the following format: what_i_want_to_predict ~ what_i_will_use_as_predictors The ~ sign is read by R as “as a function of” or “depending on”. Let’s do this and store the results in a variable called “res.lm”. It should only take a second to run. (Yes, R is finding the best line our of an infinite number of possibilities in under a second!). res.lm &lt;- lm(formula = total ~ population, data = dat) Now let’s have a look at our results: res.lm ## ## Call: ## lm(formula = total ~ population, data = dat) ## ## Coefficients: ## (Intercept) population ## -6.439 1.271 Simple and easy to read: R is telling us that our line is defined by the two numbers -6.439 and 1.271. Hence, the function of our line (the name of our line, if you will) is as follows: logmurders = -6.439 + 1.271*logpopulation You can access these two coefficients using the dollar operator ($) followed by “coefficients”: res.lm$coefficients ## (Intercept) population ## -6.439205 1.270876 Let’s plot this “best fit” line and our points together: ggplot(data=dat, aes(x=population, y=total)) + geom_point(aes(color=region)) + xlab(&quot;log10(Population size)&quot;) + ylab(&quot;log10(Number of murders)&quot;) + theme_bw() + geom_text_repel(aes(label=abb)) + ggtitle(&quot;Gun murders in 2010. Cor = 0.892&quot;) + geom_abline(intercept=res.lm$coefficients[1], slope=res.lm$coefficients[2], linetype=&quot;dashed&quot;) If we want to know more details about the linear model, we can use summary to display the key aspects of our results. summary(res.lm) ## ## Call: ## lm(formula = total ~ population, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.55966 -0.15092 0.00002 0.14696 1.09431 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.43920 0.60638 -10.62 2.64e-14 *** ## population 1.27088 0.09215 13.79 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2955 on 49 degrees of freedom ## Multiple R-squared: 0.7952, Adjusted R-squared: 0.791 ## F-statistic: 190.2 on 1 and 49 DF, p-value: &lt; 2.2e-16 Have a close look at the column named Pr(&gt;|t|). This column tells us if we have enough evidence to conclude that the prediction is valid or not. The smallest this number, the better! Finally, let’s try to predict the number of murders in some hypothetical state with 50 million inhabitants (a really big state indeed!). Don’t forget that we applied a logarithm, so we will need ot include this in the prediction. Let’s take the logarithm of 50 million: log10(50000000) ## [1] 7.69897 Now let’s feed this number to our “ideal line” equation: res.lm$coefficients[1] + res.lm$coefficients[2]*7.69897 ## (Intercept) ## 3.345233 Now let’s take the “inverse logarithm” of these number, which basically means elevating 10 to that power: 10^3.345233 ## [1] 2214.282 We predict that a state of that size would have approximately 2214 registered murders by gun. However, predictions are only estiamtes, and estimates are random. If we repeated the experiment hundreds of times, we would get multiple different results! Thus, we should always accompany a prediction with its confidence interval. We can use ggplot to plot not only the regression line, but also the cofidence interval (shown as a shade). To do this, we use geom_smooth() and specify that we want a linear model (“lm”). ggplot(data=dat, aes(x=population, y=total)) + geom_point(aes(color=region)) + geom_smooth(method = &quot;lm&quot;) + theme_bw() + geom_text_repel(aes(label=abb)) + ggtitle(&quot;Gun murders in 2010. Cor = 0.892&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; 3.8 Correlation does not equal causation Finally, let’s look at a very illustrative example: the “divorce_margarine” data set. This data set contains data on divorce rates in Maine and per capita consumption of margarine in the US. Let’s store the data in the variable “div”. div &lt;- divorce_margarine Now let’s look at the first few lines: head(div) ## divorce_rate_maine margarine_consumption_per_capita year ## 1 5.0 8.2 2000 ## 2 4.7 7.0 2001 ## 3 4.6 6.5 2002 ## 4 4.4 5.3 2003 ## 5 4.3 5.2 2004 ## 6 4.1 4.0 2005 Finally, let’s plot it and perform a linear model: ggplot(data=div, aes(x=margarine_consumption_per_capita, divorce_rate_maine)) + geom_point() + theme_bw() + xlab(&quot;Margarine consumption per capita&quot;) + ylab(&quot;Divorce rate in Maine&quot;) + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; The relationship seems very high! Let’s calculate the correlation: cor(div$divorce_rate_maine, div$margarine_consumption_per_capita) ## [1] 0.9925585 Indeed, the correlation is 0.93. An extermely high number! Based on this, would we conclude that an increase in margarine consumption causes divorce? Of course not, this doesn’t make sense! This illustrates one of the most important problems in statistics, and one of the most common soures of error. We can NEVER conclude what is the cause of what using statistics. The only thing we can say is that the variables are related, but there might be thousands of explanations for why that is. For example, maybe states which consume more margarine have a life style where divorce is more common. None of them is the cause of the other: they are both a reflection of life style. Or maybe there is no relationship at all between margarine and divorce and this is just an odd coincidence! We really cannot know. The only way to determine what is the cause of a phenomenon is making well experiments (for example, recruiting couples and feeding them margarine or some other control food, then finding if those fed with margarine divorced more often). And even then it is difficult to control for unkown factors. So the final message is the follwing: if two variables are correlated we know they might influence one another or reflect a commmon phenomenon, but we can not conclude any causality. 3.9 Exercises Now you will have some time to analyse your own project data. You can use any of the functions we learnt here by simply copying and pasting them in a new R file. You can start by askig the following questions: Take the two variables you worked with in the previous exercise. Calculate their covariance Now calculate their correlation Do they seem to follow a linear trend? If so, try to fit a linear model to them using the lm() function. Now plot the “best fit” line. Does it approximate the data well enough? If so, try to predict what some unobserved values would look like. "],["multivariate-data-analysis.html", "Chapter 4 Multivariate Data Analysis 4.1 Exploring one variable at a time 4.2 Analysing all variables jointly 4.3 Exercises 4.4 Reducing the number of dimensions 4.5 Exercises 4.6 References", " Chapter 4 Multivariate Data Analysis On previous sessions, we learnt how to analyse data containing one or two variables. We also learnt how to obtain data from public websites via web scraping. Now we are ready to take a step further and dive into data containing multiple variables. We call this type of data “multivariate”. Multivariate data can have anything from 3 or 4 to tens, hundreds or even thousands of variables. Each of these variables can be seen as a dimension, so multivarite data is also multidimensional. The majority of data that surrounds us every day is multivariate. Some examples are the following: Movement sensors in smartphones. These sensors are able to detect variables such as position, speed, acceleartion and angular movement at different points in time. Each of these measurements is a variable. Sensor data is particularly imporant in applications of internet of things (IoT). Transaction data. Banks monitor several aspects of a transaction in real time. These aspects range from location to time of the day, to amount of money moved, reference of transaction, recipient’s account and others. Banks inspect these data in real time to detect outlier transactions, which can be frauds. Genetic data. In several fields of genetics such as transcriptomics (the study of gene expression) it is common to measure the levels of tens of thousands of genes. Each gene will be recorded as a variable/dimension of our data set. Before starting this tutorial make sure to listen to the theoretical session. This session will help you picture better what multivariate data is and how it behaves, despite not being able to see it with your eyes (since our eyes can only perceive three dimensions). For completing this tutorial we will need to install three more libraries specifically designed for multivariate data analysis. You can do that by running the following code. install.packages(&quot;gplots&quot;) install.packages(&quot;corrplot&quot;) install.packages(&quot;pheatmap&quot;) Let’s now load the libraries along with some of the libraries for data analysis we studied before. library(HSAUR2) library(ggplot2) library(rafalib) library(corrplot) library(pheatmap) The data set “pottery” is part of the HSAUR2 package. It contains data from a 1980 study on specimens of very old Romano-British pottery found in archaeological sites in three different regions of Britain: Wales, Gloucester and New Forest. These pieces of pottery were analysed by atomic absorption spectrophotometry to determine their chemical composition. Chemists measured the levels of 9 different compounds in each piece of pottery (to be precise, 9 different types of oxide. Remember that, when creating pottery, clay is brought in contact with fire, and combustion is a type of oxidative reaction). They also added some information on the type of kiln (kind of oven) used to produced them. Let’s have a first look at the data: head(pottery) ## Al2O3 Fe2O3 MgO CaO Na2O K2O TiO2 MnO BaO kiln ## 1 18.8 9.52 2.00 0.79 0.40 3.20 1.01 0.077 0.015 1 ## 2 16.9 7.33 1.65 0.84 0.40 3.05 0.99 0.067 0.018 1 ## 3 18.2 7.64 1.82 0.77 0.40 3.07 0.98 0.087 0.014 1 ## 4 16.9 7.29 1.56 0.76 0.40 3.05 1.00 0.063 0.019 1 ## 5 17.8 7.24 1.83 0.92 0.43 3.12 0.93 0.061 0.019 1 ## 6 18.8 7.45 2.06 0.87 0.25 3.26 0.98 0.072 0.017 1 Since the type of kiln is a category and not, like the rest of the variables, a number, we will not consider it for now. Let’s keep the other 9 columns of this table and store them in a variable called “pots”. pots &lt;- pottery[,1:9] The “pots” data has 9 dimensions (9 variables): each dimension corresponding to the levels of a chemical compound. head(pots) ## Al2O3 Fe2O3 MgO CaO Na2O K2O TiO2 MnO BaO ## 1 18.8 9.52 2.00 0.79 0.40 3.20 1.01 0.077 0.015 ## 2 16.9 7.33 1.65 0.84 0.40 3.05 0.99 0.067 0.018 ## 3 18.2 7.64 1.82 0.77 0.40 3.07 0.98 0.087 0.014 ## 4 16.9 7.29 1.56 0.76 0.40 3.05 1.00 0.063 0.019 ## 5 17.8 7.24 1.83 0.92 0.43 3.12 0.93 0.061 0.019 ## 6 18.8 7.45 2.06 0.87 0.25 3.26 0.98 0.072 0.017 Furthermore, we have measurements for 45 pieces o pottery in this data set: dim(pots) ## [1] 45 9 We do not know which pieces come from Wales, Goucester or New Forest. Is there some way to find this out? 4.1 Exploring one variable at a time Having 9 dimensions might be overwhelming, but each of these dimensions can be studied individually using all the methods we’ve learnt before. For example, let’s create density plots for each of the 9 variables: mypar(3,3) for(i in 1:9){ plot(density(pots[,i]), main=colnames(pots)[i]) } You can see that some chemical compounds (such as barium oxide, BaO) have the characteristic “normal” (bell-shaped) distribution. Others such as calcium oxide (CaO) or Potassium oxide (K2O), however, have very unusual distributions with several modes (“bumps”). Let’s now take the first two compunds: Al2O3 and Fe2O3. Are their levels related? Do they covary? Let’s create a scatter plot to answer this question. plot(pots$Al2O3, pots$Fe2O3, xlab=&quot;Al2O3&quot;, ylab=&quot;Fe2O3&quot;) We see that for some pieces of pottery their is a possitive correlation, but for some other subgroup of pots the relation is negative. Could these come from two different locations? It is possible, but it is very difficult to conclude anything based on only 2 compounds. Let’s try to add another dimension to the previous plot. We will use the color of the points to represent the levels of our third oxide, MgO. We do this using ggplot(). ggplot(data=pots, aes(x=Al2O3, y=Fe2O3)) + geom_point(aes(color=MgO), size=3) + theme_bw() You can see that in general pottery with high levels of Fe2O3 also has high levels of MgO. But, again, it is difficult to conclude anything with only 3 compounds. 4.2 Analysing all variables jointly 4.2.1 Scatter plot matrices We cannot represent more than 3 dimensions in a plot, and even three is sometimes too confusing. However, there are some tricks that allow us to look at multiple variables in an easier way. For example, we can create scatter plots for all possible combinations of variables (in this case, all the combinations of our 9 variables). This might seem laborious. After all, we do have to create 81 different plots! However, R makes it very easy for us. We only need to use the function “pairs()”. R will automatically create plots for all possiblepairs of variables in our data set and arrange them as a matrix. We call this a scatter plot matrix. pairs(pots, pch=&quot;.&quot;) Note how the plots toward the middle (the diagonal) do not have any data on them: just the labels. This is because those plots would add no information at all: they would plot the relationship of a variable with itself, which is always a straight line. Thus, R omits them. We begin to see some patterns. For example, if you look at the Fe2O3 column you’ll realise that there are usually two groups of pottery: one with high Fe2O3 and one with low Fe2O3. However, it is difficult to find any meaningful patterns by eye when so much data is being displayed. To enhance this visulisation a little, let’s perform linear regressions for all of these combinations of variables. To do this, we use the pairs() function to create all the possible plots and we add a line (abline) with the “best fit” line (lm) for each combination of variables. In this way, R performs and plots 72 linear regressions in just one a few lines of code. pairs(pots, panel=function(x,y,...){ points(x,y,...) abline(lm(y~x), col=&quot;grey&quot;) }, pch=&quot;.&quot;) Some variables follow the line relatively well, for example MgO and K2O, but some others do not. 4.2.2 Correlation plots Just like we can plot all possible pairs of variables, we can also calculate the correlation of all possible pairs of variables in our 9-dimensional data. For example, let’s focus on the first three variables. The correlation of Al2O3 and Fe2O3 levels is: cor(pots$Al2O3, pots$Fe2O3) ## [1] -0.1396064 And the correlation between Al2O3 and MgO is: cor(pots$Al2O3, pots$MgO) ## [1] -0.7324515 Finally, the correlation between Fe2O3 and MgO levels is: cor(pots$Fe2O3, pots$MgO) ## [1] 0.3932271 We can go on for all 81 combinations. The easiest way to do this in R is simply applying the cor() function to our data. R immediately understands that what you want is to calculate not only one correlation, but all of them. These are stored in a matrix which is often called the “correlation matrix”. corrmat &lt;- cor(pots) head(corrmat) ## Al2O3 Fe2O3 MgO CaO Na2O K2O TiO2 MnO BaO ## Al2O3 1.00000000 -0.1396064 -0.7324515 0.23170316 0.01631415 -0.61126911 0.70311435 -0.5669065 0.31438288 ## Fe2O3 -0.13960637 1.0000000 0.3932271 0.66265034 0.67373208 0.61576101 -0.14583752 0.6700777 0.21126543 ## MgO -0.73245149 0.3932271 1.0000000 -0.18583362 0.14955077 0.87409305 -0.68681267 0.7836910 -0.06647407 ## CaO 0.23170316 0.6626503 -0.1858336 1.00000000 0.51609482 0.05627038 0.16486290 0.1414132 0.24928394 ## Na2O 0.01631415 0.6737321 0.1495508 0.51609482 1.00000000 0.32363840 0.04242872 0.5336162 0.36574697 ## K2O -0.61126911 0.6157610 0.8740931 0.05627038 0.32363840 1.00000000 -0.60413756 0.8504619 0.06980577 Note how the correlations we calculated above are at the top left corner of the matrix. Also note that the diagonal of the matrix contains only 1s. This is because the correlation of a variable with itself is always 1 (because they are the exact same thing!). Finally, also note that column 1 row 2 has the same numbe as column 2 row 1. The same is true for column 1 row 3 and column 3 row 1, and so on. In fact, the section of the matrix above the diagonal (the “upper section”) is exactly the same as the section below the diagonal. Just as if we had placed a mirror in the diagonal and taken the mirror image. We call these type of matrices symmetric. Correlation matrices are always symmetric. Remember that correlations go from -1 to 1. Negative correlations represent negative relationships, and the same is true for positive correlations. Thus, one way of finding out how similar each variable is to each of the other variables (how correlated they are) is by visualising this matrix. We can assign to each element of the matrix a color or the size of a circle. The plot generated is called a “correlation plot”. To create a correlation plot in R, simply use the “corrplot()” function and specify where your correlations are stored, as follows. corrplot(corrmat) Notice how the diagonal is full of ones (dark blue). The shading and the size of the circle are proportional to the correlation. This plot is also symetric, because correlation matrices are symmetric. Thus, we can eliminate one half of it without losing any information. Let’s keep only the upper part: corrplot(corrmat, type = &quot;upper&quot;) Now we can at last start making sense of this multivariate data. We see, for example, that levels of Fe2O3 are correlated with almost everything else. When Fe2O3 increases, CaO also icreases, as well as Na2O and MnO. However, MgO and Al2O3 have a negative correlation: when one increses the other decreases. What else can you see? 4.2.3 The distance matrix As said before (and as discussed in previous sessios) each variable in our data set can be seen as a “dimension”. This has to do with data having “geomtric” properties. What this means is that our data set can be seen as a “space of 9 dimensions” where each of the pottery specimens is a dot with precise coordinates. Because each row in our matrix can be thought of as a “point in space”, we can actually calculate the distance between any two points. This distance is impossible to visualise or imagine by human minds, because our own world only has a few dimensions and not 9, but this distance exists mathematically! To calculate the distance between the pottery specimen 1 and 2 in this 9-dimensional space, for instance, we use the equation for Euclidean distances. This equation is one of the fundamentals of analytical geometry and comes from the pythagorean theorem, but we will not explain it in depth here. The distance between pottery 1 and 2 is the following: sqrt(sum((pots[1,1:9] - pots[2,1:9])^2)) ## [1] 2.924741 And the distance between specimen 1 and 3 is as follows: sqrt(sum((pots[1,1:9] - pots[3,1:9])^2)) ## [1] 1.986228 Just like we did above with correlations, we can calculate the distance between all possible pairs of pottery specimens. These distances are, again, arrange into a matrix we call the “distance matrix”. R has an easy function to do that in just one line The function is called “dists()” and you only need to specify where your data is stored, as follows: distmat &lt;- dist(pots) Let’s look at the first 6 rows and columns of this distance matrix as.matrix(distmat)[1:6,1:6] ## 1 2 3 4 5 6 ## 1 0.000000 2.9247408 1.9862278 2.9665151 2.5016339 2.0789250 ## 2 2.924741 0.0000000 1.3493762 0.1273460 0.9307723 1.9647203 ## 3 1.986228 1.3493762 0.0000000 1.3717146 0.5909323 0.7228651 ## 4 2.966515 0.1273460 1.3717146 0.0000000 0.9600542 1.9911768 ## 5 2.501634 0.9307723 0.5909323 0.9600542 0.0000000 1.0743021 ## 6 2.078925 1.9647203 0.7228651 1.9911768 1.0743021 0.0000000 Just like with the correlation, the distance matrix is symetric. This is because the distance from speciment 1 to specimen 2 is the exact same as the distance between specimen 2 and speciment 1. Also, note that the diagonal is made of zeros. This is because the distance between a point and itself is always zero: they occupy the same position in space! 4.2.4 Clustering and dendrograms We can use the matrix of distances to find which points are close to each other in space, and which are seaprate. What this tells us in the real world is which pieces of pottery are very similar to each other, and which are different. This process of identyfing “subgroups” or “substructures” in the data is called “clustering”. One of the most common methods for clustering data is “hierarchical clustering”“, which works by building a dendrogram. A dendrogram is a type of tree (just like a family tree). In our case, we will build a tree in which each pot is closest to the most similar pot and farthest from the most dissimlar pot. To do this in R we use the function hclust(). Let’s store the results in the variable “tree”. tree &lt;- hclust(distmat) Now let’s plot the results: plot(tree) This plot tells us a lot about the data! In fact, you can see that there are three groups (three big branches in the tree). We call these groups “clusters”. Because we know that the pottery used for this study comes from three different areas of the UK, we can postulate that each of these three clusters may correspond to a different location. We have found which pots belong to which region! 4.2.5 Heatmaps However, in the tree above all we know is which pots belong to which group (or region). We do not know what characterises each pot: what makes Gloucester pottery different from Wales pottery? We cannot know. A way to gain more insights into this is by using a very creative type of visualisation: “heatmaps”. Heatmaps are dendrograms (trees) in which, for each individual element (in this case, each piece of pottery), we also get to see the value of all the variables (in this case the levels of each chemical element). You might be surprised this is possible at all. Above, we tried all sorts of methods to look at this data and yet we could not see more than 3 variables at a time! So how do heatmaps achieve this? The answer is that they cheat! They “deceive” our eyes. The way they do this is by assigning an order to colours. Colours do not have any natural order (blue is not bigger than green or yellow), but we can order them according to some scheme; for example, their position in the rainbow. A common practice is to say that blue is “lower” than “red” because blue is cold and red is hot (this is why they are called heatmaps). This is, of couse, arbitrary but it works surprisingly well! Now we can visualise each chemical element in each pot as a very thin stripe of colour. Because the pieces of pottery (and, in fact, the chemical compounds too) are ordered using a tree (hierarchical clustering) things which are similar will be close to each other in the plot. Putting similar things together helps our eyes detect patterns much better. In fact, patterns are amplified by the heatmap for us to see them. Let’s use he pheatmap() funciton (phemap stands for “Pretty HEATMAPs”) to create a heatmap of our data. We will set the colours to go from blue (cold) to red (hot). mycolors &lt;- colorRampPalette(c(&quot;blue&quot;,&quot;white&quot;,&quot;red&quot;))(1000) pheatmap(pots, scale = &quot;column&quot;, col=mycolors, fontsize_row = 5) Now we are getting a lot of useful information out of the data! We see that there are two clear groups of pots (which we believe correspond to the regions where the pottery comes from). Pots of the first region have relatively high levels of everything except aluminium and titanium oxides (Al2O3, Ti2O3). Pots of the second region contain high levels of magnesium, manganesum and potasium ozides (MnO, MgO, K2O) but lower levels of everyting else. Finally, pots from the third region contain high levels of almost everything (especially calcium oxide), but low levels of MnO, MgO and K2O. 4.3 Exercises Before going into the next section, you will have time to work on your project data. You can start by asking the following: How many variables on your project data set are numerical? Which are these? You can focus on them. These will include the variables you looked at in previous sessions For all these variables, create a scatter plot matrix. If possible, add a “best fit” line Calculate the correlation and distnace matrices Plot the correlations as a correlation plot. Which variables are related to each other? Use the distance matrix to perform clustering and create a tree. Do you have any subgroups in your data? Create a heatmap. What does this heatmap tell you about your data? Is this something you could’ve learnt from analysing the variables separately? 4.4 Reducing the number of dimensions In the previous sections we used different techniques to analyse all the 9 variables in our data. However, sometimes this is impossible to do. What if we don’t have 9 or 10 or 15 but 150 or 15,000 variables? In big data this is common (hence the “big” in big data). There is another way of analysing multiple variables in which, instead of analysing all of them, we “reduce them”. This means, we use the 9 or 10 or 100 dimensions we have to create just 2 or 3 new (and better) dimensions. These new dimensions are combinations of the original ones. This is analogous to creating new (better) axes by rotating our original axes around. In the theoretical part of this session you will learn in more detail what this means and how dimensions can be reduced. However, the mathematics behind this are quite complicated and thus will not be explained here. The best known method for reducing dimensions is called “Principal Component Analysis” (PCA). To perform PCA in R you can use the function “prcomp()”. Let’s perform PCA on our data: PCs &lt;- prcomp(pots, scale. = T) Now let’s have a look at the results: summary(PCs) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 ## Standard deviation 2.0503 1.5885 0.93699 0.67538 0.61647 0.51840 0.34325 0.30190 0.2846 ## Proportion of Variance 0.4671 0.2804 0.09755 0.05068 0.04223 0.02986 0.01309 0.01013 0.0090 ## Cumulative Proportion 0.4671 0.7475 0.84501 0.89570 0.93792 0.96778 0.98087 0.99100 1.0000 You will see that now, instead of the original 9 chemical compunds, we have 9 new dimensions. These dimensions are called “Principal Components” (PCs). You might think that this is not really a reduction, because we had 9 dimensions originally and we ended up with another 9! However, PCs are ordered by importance and most of the times you will not need more than the first two or three of them. For example, in this case PC1 and PC2 together already account for 74% (0.46 + 0.28) of the variance (differences we see) in our data. So plotting these two dimensions is almost equivalent to plotting the 9 original ones. Let’s plot the first two PCs: plot(PCs$x, pch=19) We immediately see there are three groups of samples, which are probably the same we identified before with hierarchical clustering. You might be wondering what each of these PCs mean. What is a PC? How is it related to any of the chemical compounds we originally had? The answer depends on the data: PCs will have a different meaning for every data set you work with. Let’s find out what they mean in this specific example. Above I mentioned that each PC is a different “combination” of the original 9 variables, in the output below we can see how much each compund is contributing to each of the PCs. PCs ## Standard deviations (1, .., p=9): ## [1] 2.0503433 1.5884850 0.9369854 0.6753828 0.6164727 0.5183982 0.3432531 0.3019006 0.2845736 ## ## Rotation (n x k) = (9 x 9): ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 ## Al2O3 -0.34829631 0.32780562 0.119014979 -0.03608198 0.32219091 -0.776136871 -0.017576097 0.21945465 -0.03319629 ## Fe2O3 0.32709620 0.39525949 -0.264444108 -0.02114131 0.34343318 -0.045613182 0.243749457 -0.50439926 0.48222510 ## MgO 0.43456923 -0.18964741 0.150918366 0.05350702 0.28126791 -0.009243254 -0.442140420 0.49189525 0.48259499 ## CaO 0.06428531 0.50119655 -0.477928669 -0.49719394 -0.06858469 0.226593748 -0.171170581 0.39348466 -0.16992250 ## Na2O 0.21717586 0.45551237 -0.007020976 0.57759974 -0.53034971 -0.157645754 -0.321107660 -0.04474717 -0.02090327 ## K2O 0.45633257 -0.01837517 0.102097013 -0.03872041 0.38892815 -0.078842428 -0.309777858 -0.28554772 -0.66672244 ## TiO2 -0.34019754 0.30078403 0.089583383 0.49161094 0.49299846 0.520801535 -0.005608328 0.14702257 -0.09032883 ## MnO 0.45522720 0.08753781 0.140202878 0.15303057 -0.02199131 -0.048867577 0.718184163 0.42728179 -0.20199070 ## BaO 0.01854192 0.37839957 0.791553826 -0.38466339 -0.13551441 0.198520122 -0.024441118 -0.11359903 0.10329608 For example, PC2 (the vertical axis in our previous plot), which distinguishes one of the pottery regions from the other two, is mostly a combination of high CaO and Na2O, but low MgO and K2O. This probably corresponds to region number two in our heatmap (see above). 4.4.1 Combining dimensionality reduction with clustering Now let’s combine both of the approaches we’ve learnt. We first used the distance between pottery specimens to build a dendrogram or tree, in which we saw three different groups. This is our tree: plot(tree, main=&quot;Dendrogram of pottery samples&quot;, xlab=&quot;&quot;) Then we use PCA to reduce the dimensions and we also found three groups. These are the results: plot(PCs$x, pch=19) Are these two results equivalent? Do they indicate the same 3 groups? To verify this, let’s recover the three groups in our tree (dendrogram). If you look at the tree you’ll see that, if you “cut”/“slice” the branches of the tree close to height = 8, you get the tree groups. Let’s use the function “cutree()” to do this and store the results: pot.types &lt;- cutree(tree, h=8) The result of this is a list of numbers which tell us which pot specimen belongs to which group (groups are 1, 2 and 3). pot.types ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 ## 1 1 1 1 1 1 1 1 1 2 2 2 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 Now let’s draw the PCA plot again, but adding these labels. Group 1 will be represented by points, group 2 by triangles and group 3 by crosses. ggplot(data=as.data.frame(PCs$x), aes(PC1,PC2)) + geom_point(shape=pot.types, size=3) + theme_bw() We see that the conclusions from both methods are ALMOST the same, though there are only 4 pieces of pottery that are classified into different groups/regions by the two methods. Now you know how to reduce the dimensions of a data using PCA. This might still be confusing since dimensionality reduction is a difficult concept to grasp. However, in the next sessions there will be more exercises to help you understand how PCA is used to analyse multi-dimensional data. 4.5 Exercises You will now have some time to explore your project data. A good starting point would be: Take the numeric variables you were working with before and perform PCA on them Look at the PCs: how many will you need to summarise your data? Plot the first two PCs. What do they tell you about the data? Do you need to plot any other PCs? Look at the contribution of each variable to your PCs. What does each of your PCs mean? 4.6 References This session was based on fragments from the following book: Everitt B, Hothorn T. (2011). An Introduction to applied multivariate analysis with R. London: Springer. A. Tubb and N. J. Parker and G. Nickless (1980), The analysis of Romano-British pottery by atomic absorption spectrophotometry. Archaeometry, 22, 153–171. "],["web-scraping-in-r.html", "Chapter 5 Web scraping in R 5.1 Example: Scraping the IMDB website 5.2 Extracting variables using CSS selectors 5.3 Scraping multiple fields from a website 5.4 Performing exploratory data analysis (EDA) on scraped data 5.5 Exercises 5.6 References", " Chapter 5 Web scraping in R During this session, we will learn how to use R to perform “web scraping”. Web scraping, aslfo called web harvesting, is the extraction of data from websites in an ordered and structured manner. We have all done web scraping in its simplest form: simply “copying and pasting”. However, sometimes the information on a website is extremely unstructured (buried in the text or in formats which are difficult to interpret), and it would be time consuming to copy and paste all the information we need by hand. Furthermore, often we need to scrape not one but tends or hundreds of aspects of a website; and often this has to be repeated for multiple websites too. Hence, developing methods for atuomated web scraping is central in big data and data analysis. The R package “rvest” can be used for web scraping. Let’s first install it. We will also install “wordcloud”, which can be used for processing text data. install.packages(&quot;rvest&quot;) install.packages(&quot;wordcloud&quot;) Now let’s load it. We will also load the ggplot2 and ggrepel packages for data visualisation. library(rvest) library(ggplot2) library(ggrepel) library(wordcloud) For eficient web scraping, you will need the “Selector Gadget”. Make sure you have a working version of Google Chrome installed. Next, use Chrome to open the following link: https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=en Finally, click the “Add to chrome” buttom, it should only take a few seconds. Now the Selector Gadget is ready to use. 5.1 Example: Scraping the IMDB website For the purpose of this tutorial, we will be scraping the IMDB website, which contains raiting and information for films. Let’s focus specifically on the following website, which contains the 100 most popular films released between January and December 2016: http://www.imdb.com/search/title?count=100&amp;release_date=2016,2016&amp;title_type=feature Take some time to explore the website and its metrics. Notice that, for each film, IMDB contains information such as its title, portrait, rating, staring actors, rank, genre, duration, etc… The first fact we have to realise is that websites are simply files, just as our word files or spreadsheets are files. They are files written in the “hyper-text markup language” (html). Every time we access a website, we specify our computer the location of such file via a “Uniform resource locator” (URL) and the computer displays the file using a browser (Chrome, Safari, Firefox, etc…). Browsers simply translate the HTML language to something easy for us to visualise: an arrangement of images, colours, text, links, and other objects. We can easily open html files in R just as we have previously opened spreadsheets (CSV files), text files or tables. We use the function “read_html()”, which is part of the rvest package. Let’s read the top 100 2016 IMDB films website into R. We’ll assign the content to a variable called “webpage”. webpage &lt;- read_html(&quot;http://www.imdb.com/search/title?count=100&amp;release_date=2016,2016&amp;title_type=feature&quot;) Let’s look at the content of “webpage”: webpage ## {html_document} ## &lt;html xmlns:og=&quot;http://ogp.me/ns#&quot; xmlns:fb=&quot;http://www.facebook.com/2008/fbml&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;\\n&lt;script type=&quot;text/javascript&quot;&gt;var ue_t0=ue_t0||+new Date();&lt;/script&gt;&lt;script type=&quot;text/jav ... ## [2] &lt;body id=&quot;styleguide-v2&quot; class=&quot;fixed&quot;&gt;\\n &lt;img height=&quot;1&quot; width=&quot;1&quot; style=&quot;display:none;visibility:hidden;&quot; src=&quot;//fls-na.amazon.com/1/batch/1/OP/A1EVAM02EL8 ... Note that it is an xml document and that it contains several fields, written in HTML (HTML is a language characterised by a lot of &lt; &gt; symbols). Inside this variable is contained all the information we can see when scrolling in our browser. 5.2 Extracting variables using CSS selectors Now let’ move to Chrome. We will use Chrome’s “Selector Gadget”” to retrieve specific fields/variables from the website. To activate the selector gadget, click on the magnifying glass at the top right corner of the browser. Now, if you move your pointer around the website, you will see different sections being highlighted. Find any section that interests you (for example, one of the pirctureS) and click on it. You will see that a string of text appears in the bottom right corner of the screen. This text is called the “CSS selector”. You can think of CSS selectors as “unique identifiers” of specific elements on a website. For example, you’ll notice that when you clicked the film’s image all the other images were also highlighted. This is because that CSS elector identifies ALL the film covers. This is useful because we can use this text to extract all of them, instead of copying and pasting 100 images one by one. Let’s apply this to web scraping. Using the Selector Gadget, click on the title of the first film. You should see the entirety of he title highlighted in yellow. The text “.lister-item-header” will appear at the bottom right. This is the CSS selector for movie titles. Copy that text. Now use the function “html_nodes” and specify that you want to extract the film titles by pasting the CSS selector, as follows: title_data_html &lt;- html_nodes(webpage,&#39;.lister-item-header a&#39;) The titles have now been scraped! Let’s see how they look: head(title_data_html) ## {xml_nodeset (6)} ## [1] &lt;a href=&quot;/title/tt1431045/?ref_=adv_li_tt&quot;&gt;Deadpool&lt;/a&gt; ## [2] &lt;a href=&quot;/title/tt3748528/?ref_=adv_li_tt&quot;&gt;Rogue One&lt;/a&gt; ## [3] &lt;a href=&quot;/title/tt1386697/?ref_=adv_li_tt&quot;&gt;Suicide Squad&lt;/a&gt; ## [4] &lt;a href=&quot;/title/tt1211837/?ref_=adv_li_tt&quot;&gt;Doctor Strange&lt;/a&gt; ## [5] &lt;a href=&quot;/title/tt3783958/?ref_=adv_li_tt&quot;&gt;La La Land&lt;/a&gt; ## [6] &lt;a href=&quot;/title/tt3799694/?ref_=adv_li_tt&quot;&gt;The Nice Guys&lt;/a&gt; We cannot understand anything because they are in HTML language! Let’s convert them to normal textusing the function “html_text()”. We’ll store these titles in the variable “title_data”. title_data &lt;- html_text(title_data_html) Now they are understandable to us: title_data ## [1] &quot;Deadpool&quot; &quot;Rogue One&quot; &quot;Suicide Squad&quot; ## [4] &quot;Doctor Strange&quot; &quot;La La Land&quot; &quot;The Nice Guys&quot; ## [7] &quot;Moana&quot; &quot;Hacksaw Ridge&quot; &quot;Sing&quot; ## [10] &quot;Arrival&quot; &quot;Captain America: Civil War&quot; &quot;Nocturnal Animals&quot; ## [13] &quot;The Handmaiden&quot; &quot;X-Men: Apocalypse&quot; &quot;Split&quot; ## [16] &quot;10 Cloverfield Lane&quot; &quot;Fantastic Beasts and Where to Find Them&quot; &quot;Me Before You&quot; ## [19] &quot;Zootropolis&quot; &quot;War Dogs&quot; &quot;Hell or High Water&quot; ## [22] &quot;High Strung&quot; &quot;Batman v Superman: Dawn of Justice&quot; &quot;Train to Busan&quot; ## [25] &quot;Your Name.&quot; &quot;Gods of Egypt&quot; &quot;Passengers&quot; ## [28] &quot;Hidden Figures&quot; &quot;Captain Fantastic&quot; &quot;Jason Bourne&quot; ## [31] &quot;13 Hours&quot; &quot;The Huntsman: Winter&#39;s War&quot; &quot;Below Her Mouth&quot; ## [34] &quot;Star Trek Beyond&quot; &quot;Ghostbusters&quot; &quot;Central Intelligence&quot; ## [37] &quot;Don&#39;t Breathe&quot; &quot;Manchester by the Sea&quot; &quot;The Conjuring 2&quot; ## [40] &quot;The Neon Demon&quot; &quot;The Accountant&quot; &quot;Swiss Army Man&quot; ## [43] &quot;Moonlight&quot; &quot;Miss Peregrine&#39;s Home for Peculiar Children&quot; &quot;The Shallows&quot; ## [46] &quot;Lion&quot; &quot;Independence Day: Resurgence&quot; &quot;Assassin&#39;s Creed&quot; ## [49] &quot;Terrifier&quot; &quot;Hush&quot; &quot;Warcraft: The Beginning&quot; ## [52] &quot;The Magnificent Seven&quot; &quot;Raw&quot; &quot;Silence&quot; ## [55] &quot;Sausage Party&quot; &quot;Love Machine&quot; &quot;Allied&quot; ## [58] &quot;Why Him?&quot; &quot;The Founder&quot; &quot;A Cure for Wellness&quot; ## [61] &quot;Jack Reacher: Never Go Back&quot; &quot;Hunt for the Wilderpeople&quot; &quot;Dirty Grandpa&quot; ## [64] &quot;The Wailing&quot; &quot;The Lost City of Z&quot; &quot;The Autopsy of Jane Doe&quot; ## [67] &quot;The Jungle Book&quot; &quot;Allegiant&quot; &quot;The Girl on the Train&quot; ## [70] &quot;Teenage Mutant Ninja Turtles: Out of the Shadows&quot; &quot;The Invisible Guest&quot; &quot;Batman v Superman: Dawn of Justice - Ultimate Edition&quot; ## [73] &quot;Resident Evil: The Final Chapter&quot; &quot;The Great Wall&quot; &quot;My Big Fat Greek Wedding 2&quot; ## [76] &quot;The Legend of Tarzan&quot; &quot;Sully: Miracle on the Hudson&quot; &quot;The Edge of Seventeen&quot; ## [79] &quot;The Bad Batch&quot; &quot;Now You See Me 2&quot; &quot;Handsome Devil&quot; ## [82] &quot;The Secret Life of Pets&quot; &quot;Inferno&quot; &quot;Kung Fu Panda 3&quot; ## [85] &quot;The 5th Wave&quot; &quot;Lights Out&quot; &quot;Grimsby&quot; ## [88] &quot;London Has Fallen&quot; &quot;The Boy&quot; &quot;Everybody Wants Some!!&quot; ## [91] &quot;Lady Macbeth&quot; &quot;A Silent Voice&quot; &quot;Finding Dory&quot; ## [94] &quot;Deepwater Horizon&quot; &quot;Brimstone&quot; &quot;The Take&quot; ## [97] &quot;Jackie&quot; &quot;How to Be Single&quot; &quot;Hail, Caesar!&quot; ## [100] &quot;Keanu&quot; 5.3 Scraping multiple fields from a website A) IMDB ranking: Now we have the titles for the top 100 2016 IMDB films, but if we want to do any serious analysis on them we will need more data. Let’s repeat the previous process and use CSS selectors to scrap more fields. We start by scraping the rankings: rank_data_html &lt;- html_nodes(webpage,&#39;.text-primary&#39;) We convert the rankings from html to text in the same way we did above: rank_data &lt;- html_text(rank_data_html) However, note that the rankings shouldn’t really be text, but numbers. head(rank_data) ## [1] &quot;1.&quot; &quot;2.&quot; &quot;3.&quot; &quot;4.&quot; &quot;5.&quot; &quot;6.&quot; Let’s use as.numeric() to convert them into nummeric variables. We’ll store this in the variable “rank_data”. rank_data &lt;- as.numeric(rank_data) Now they look better. rank_data ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 ## [43] 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 ## [85] 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 B) Film duration: Let’s repeat the same process to extrac the duration of each film, which in IMDB is called “runtime”. runtime_data_html &lt;- html_nodes(webpage,&#39;.runtime&#39;) runtime_data &lt;- html_text(runtime_data_html) runtime_data ## [1] &quot;108 min&quot; &quot;133 min&quot; &quot;123 min&quot; &quot;115 min&quot; &quot;128 min&quot; &quot;116 min&quot; &quot;107 min&quot; &quot;139 min&quot; &quot;108 min&quot; &quot;116 min&quot; &quot;147 min&quot; &quot;116 min&quot; &quot;145 min&quot; &quot;144 min&quot; &quot;117 min&quot; &quot;103 min&quot; &quot;132 min&quot; ## [18] &quot;106 min&quot; &quot;108 min&quot; &quot;114 min&quot; &quot;102 min&quot; &quot;96 min&quot; &quot;152 min&quot; &quot;118 min&quot; &quot;106 min&quot; &quot;127 min&quot; &quot;116 min&quot; &quot;127 min&quot; &quot;118 min&quot; &quot;123 min&quot; &quot;144 min&quot; &quot;114 min&quot; &quot;94 min&quot; &quot;122 min&quot; ## [35] &quot;117 min&quot; &quot;107 min&quot; &quot;88 min&quot; &quot;137 min&quot; &quot;134 min&quot; &quot;117 min&quot; &quot;128 min&quot; &quot;97 min&quot; &quot;111 min&quot; &quot;127 min&quot; &quot;86 min&quot; &quot;118 min&quot; &quot;120 min&quot; &quot;115 min&quot; &quot;82 min&quot; &quot;82 min&quot; &quot;123 min&quot; ## [52] &quot;132 min&quot; &quot;99 min&quot; &quot;161 min&quot; &quot;89 min&quot; &quot;79 min&quot; &quot;124 min&quot; &quot;111 min&quot; &quot;115 min&quot; &quot;146 min&quot; &quot;118 min&quot; &quot;101 min&quot; &quot;102 min&quot; &quot;156 min&quot; &quot;141 min&quot; &quot;86 min&quot; &quot;106 min&quot; &quot;120 min&quot; ## [69] &quot;112 min&quot; &quot;112 min&quot; &quot;106 min&quot; &quot;182 min&quot; &quot;107 min&quot; &quot;103 min&quot; &quot;94 min&quot; &quot;110 min&quot; &quot;96 min&quot; &quot;104 min&quot; &quot;118 min&quot; &quot;129 min&quot; &quot;95 min&quot; &quot;87 min&quot; &quot;121 min&quot; &quot;95 min&quot; &quot;112 min&quot; ## [86] &quot;81 min&quot; &quot;83 min&quot; &quot;99 min&quot; &quot;97 min&quot; &quot;117 min&quot; &quot;89 min&quot; &quot;130 min&quot; &quot;97 min&quot; &quot;107 min&quot; &quot;148 min&quot; &quot;92 min&quot; &quot;100 min&quot; &quot;110 min&quot; &quot;106 min&quot; &quot;100 min&quot; Again, this variable isn’t really a text but a number (number of minutes). Let’s first remove the text ” minute” from every element using gsub(). runtime_data &lt;- gsub(&quot; min&quot;,&quot;&quot;,runtime_data) Now let’s transform the text to nummeric and store it in the variable “runtime_data”. runtime_data&lt;-as.numeric(runtime_data) It looks much better! runtime_data ## [1] 108 133 123 115 128 116 107 139 108 116 147 116 145 144 117 103 132 106 108 114 102 96 152 118 106 127 116 127 118 123 144 114 94 122 117 107 88 137 134 117 128 97 ## [43] 111 127 86 118 120 115 82 82 123 132 99 161 89 79 124 111 115 146 118 101 102 156 141 86 106 120 112 112 106 182 107 103 94 110 96 104 118 129 95 87 121 95 ## [85] 112 81 83 99 97 117 89 130 97 107 148 92 100 110 106 100 C) Film genre: Now let’s scrape the genre of each movie. genre_data_html &lt;- html_nodes(webpage,&#39;.genre&#39;) genre_data &lt;- html_text(genre_data_html) head(genre_data) ## [1] &quot;\\nAction, Adventure, Comedy &quot; &quot;\\nAction, Adventure, Sci-Fi &quot; &quot;\\nAction, Adventure, Fantasy &quot; &quot;\\nAction, Adventure, Fantasy &quot; ## [5] &quot;\\nComedy, Drama, Music &quot; &quot;\\nAction, Comedy, Crime &quot; There are several problems here: firstly, there is a lot of extra space. Secondly, all the genres start with the new line () symbol. Finally, there are multiple genres for each film. Let’s fix these issues one by one. First, we remove the new line symbols using gsub(). genre_data&lt;-gsub(&quot;\\n&quot;,&quot;&quot;,genre_data) Next, we do the same to remove empty spaces. genre_data&lt;-gsub(&quot; &quot;,&quot;&quot;,genre_data) Finally, let’s use gsub() to remove all the additional genres. We only keep the first one for each film. genre_data&lt;-gsub(&quot;,.*&quot;,&quot;&quot;,genre_data) genre_data ## [1] &quot;Action&quot; &quot;Action&quot; &quot;Action&quot; &quot;Action&quot; &quot;Comedy&quot; &quot;Action&quot; &quot;Animation&quot; &quot;Biography&quot; &quot;Animation&quot; &quot;Drama&quot; &quot;Action&quot; &quot;Drama&quot; &quot;Drama&quot; &quot;Action&quot; ## [15] &quot;Horror&quot; &quot;Drama&quot; &quot;Adventure&quot; &quot;Drama&quot; &quot;Animation&quot; &quot;Biography&quot; &quot;Crime&quot; &quot;Drama&quot; &quot;Action&quot; &quot;Action&quot; &quot;Animation&quot; &quot;Action&quot; &quot;Drama&quot; &quot;Biography&quot; ## [29] &quot;Comedy&quot; &quot;Action&quot; &quot;Action&quot; &quot;Action&quot; &quot;Drama&quot; &quot;Action&quot; &quot;Action&quot; &quot;Action&quot; &quot;Crime&quot; &quot;Drama&quot; &quot;Horror&quot; &quot;Horror&quot; &quot;Action&quot; &quot;Comedy&quot; ## [43] &quot;Drama&quot; &quot;Adventure&quot; &quot;Action&quot; &quot;Biography&quot; &quot;Action&quot; &quot;Action&quot; &quot;Horror&quot; &quot;Horror&quot; &quot;Action&quot; &quot;Action&quot; &quot;Drama&quot; &quot;Drama&quot; &quot;Animation&quot; &quot;Drama&quot; ## [57] &quot;Action&quot; &quot;Comedy&quot; &quot;Biography&quot; &quot;Drama&quot; &quot;Action&quot; &quot;Adventure&quot; &quot;Comedy&quot; &quot;Drama&quot; &quot;Adventure&quot; &quot;Horror&quot; &quot;Adventure&quot; &quot;Action&quot; &quot;Crime&quot; &quot;Action&quot; ## [71] &quot;Crime&quot; &quot;Action&quot; &quot;Action&quot; &quot;Action&quot; &quot;Comedy&quot; &quot;Action&quot; &quot;Biography&quot; &quot;Comedy&quot; &quot;Action&quot; &quot;Action&quot; &quot;Comedy&quot; &quot;Animation&quot; &quot;Action&quot; &quot;Animation&quot; ## [85] &quot;Action&quot; &quot;Horror&quot; &quot;Action&quot; &quot;Action&quot; &quot;Horror&quot; &quot;Comedy&quot; &quot;Drama&quot; &quot;Animation&quot; &quot;Animation&quot; &quot;Action&quot; &quot;Drama&quot; &quot;Action&quot; &quot;Biography&quot; &quot;Comedy&quot; ## [99] &quot;Comedy&quot; &quot;Action&quot; But perhaps text is not the best way to store this information. It is, in the end, a list with categories. It would be much better to have this as a factor variable. We thus convert it into a factor. genre_data&lt;-as.factor(genre_data) Now it looks much better! genre_data ## [1] Action Action Action Action Comedy Action Animation Biography Animation Drama Action Drama Drama Action Horror Drama Adventure ## [18] Drama Animation Biography Crime Drama Action Action Animation Action Drama Biography Comedy Action Action Action Drama Action ## [35] Action Action Crime Drama Horror Horror Action Comedy Drama Adventure Action Biography Action Action Horror Horror Action ## [52] Action Drama Drama Animation Drama Action Comedy Biography Drama Action Adventure Comedy Drama Adventure Horror Adventure Action ## [69] Crime Action Crime Action Action Action Comedy Action Biography Comedy Action Action Comedy Animation Action Animation Action ## [86] Horror Action Action Horror Comedy Drama Animation Animation Action Drama Action Biography Comedy Comedy Action ## Levels: Action Adventure Animation Biography Comedy Crime Drama Horror D) IMDB rating: We use CSS selectors to scrap the IMDB rating section. Then, we convert these ratings to numbers. rating_data_html &lt;- html_nodes(webpage,&#39;.ratings-imdb-rating strong&#39;) rating_data &lt;- html_text(rating_data_html) rating_data&lt;-as.numeric(rating_data) rating_data ## [1] 8.0 7.8 5.9 7.5 8.0 7.3 7.6 8.1 7.1 7.9 7.8 7.5 8.1 6.9 7.3 7.2 7.2 7.4 8.0 7.1 7.6 6.3 6.4 7.6 8.4 5.4 7.0 7.8 7.8 6.6 7.3 6.1 5.5 7.0 6.9 6.3 7.1 7.8 7.3 6.1 7.3 6.9 ## [43] 7.4 6.7 6.3 8.0 5.2 5.6 5.6 6.6 6.7 6.8 7.0 7.1 6.1 4.0 7.1 6.2 7.2 6.4 6.1 7.8 5.8 7.4 6.6 6.8 7.4 5.6 6.5 5.9 8.0 7.1 5.5 5.9 6.0 6.2 7.4 7.3 5.2 6.4 7.0 6.5 6.2 7.1 ## [85] 5.2 6.3 6.2 5.9 6.0 6.9 6.8 8.1 7.3 7.1 7.0 6.3 6.7 6.0 6.3 6.2 E) Film description: Finally, we use CSS selectors to retrieve the description of each film. Each description is a paragraph summarising the content of the film. description_data_html &lt;- html_nodes(webpage,&#39;.ratings-bar+ .text-muted&#39;) description_data &lt;- html_text(description_data_html) head(description_data) ## [1] &quot;\\nA wisecracking mercenary gets experimented on and becomes immortal but ugly, and sets out to track down the man who ruined his looks.&quot; ## [2] &quot;\\nIn a time of conflict, a group of unlikely heroes band together on a mission to steal the plans to the Death Star, the Empire&#39;s ultimate weapon of destruction.&quot; ## [3] &quot;\\nA secret government agency recruits some of the most dangerous incarcerated super-villains to form a defensive task force. Their first mission: save the world from the apocalypse.&quot; ## [4] &quot;\\nWhile on a journey of physical and spiritual healing, a brilliant neurosurgeon is drawn into the world of the mystic arts.&quot; ## [5] &quot;\\nWhile navigating their careers in Los Angeles, a pianist and an actress fall in love while attempting to reconcile their aspirations for the future.&quot; ## [6] &quot;\\nIn 1970s Los Angeles, a mismatched pair of private eyes investigate a missing girl and the mysterious death of a porn star.&quot; We remove the new line symbol and the extra spaces. description_data&lt;-gsub(&quot;\\n&quot;,&quot;&quot;,description_data) description_data&lt;-gsub(&quot; &quot;,&quot;&quot;,description_data) head(description_data) ## [1] &quot;A wisecracking mercenary gets experimented on and becomes immortal but ugly, and sets out to track down the man who ruined his looks.&quot; ## [2] &quot;In a time of conflict, a group of unlikely heroes band together on a mission to steal the plans to the Death Star, the Empire&#39;s ultimate weapon of destruction.&quot; ## [3] &quot;A secret government agency recruits some of the most dangerous incarcerated super-villains to form a defensive task force. Their first mission: save the world from the apocalypse.&quot; ## [4] &quot;While on a journey of physical and spiritual healing, a brilliant neurosurgeon is drawn into the world of the mystic arts.&quot; ## [5] &quot;While navigating their careers in Los Angeles, a pianist and an actress fall in love while attempting to reconcile their aspirations for the future.&quot; ## [6] &quot;In 1970s Los Angeles, a mismatched pair of private eyes investigate a missing girl and the mysterious death of a porn star.&quot; Now that we have scraped all of those variables, we can put them together into one single table. Let’s use the data.frame() function to build a table where each of the variables we scraped before will become a column: films &lt;- data.frame(Title=title_data, Genre=genre_data, Runtime=runtime_data, Rank=rank_data, IMDB_rating=rating_data, Description=description_data) Let’s have a look at the data set. head(films) ## Title Genre Runtime Rank IMDB_rating ## 1 Deadpool Action 108 1 8.0 ## 2 Rogue One Action 133 2 7.8 ## 3 Suicide Squad Action 123 3 5.9 ## 4 Doctor Strange Action 115 4 7.5 ## 5 La La Land Comedy 128 5 8.0 ## 6 The Nice Guys Action 116 6 7.3 ## Description ## 1 A wisecracking mercenary gets experimented on and becomes immortal but ugly, and sets out to track down the man who ruined his looks. ## 2 In a time of conflict, a group of unlikely heroes band together on a mission to steal the plans to the Death Star, the Empire&#39;s ultimate weapon of destruction. ## 3 A secret government agency recruits some of the most dangerous incarcerated super-villains to form a defensive task force. Their first mission: save the world from the apocalypse. ## 4 While on a journey of physical and spiritual healing, a brilliant neurosurgeon is drawn into the world of the mystic arts. ## 5 While navigating their careers in Los Angeles, a pianist and an actress fall in love while attempting to reconcile their aspirations for the future. ## 6 In 1970s Los Angeles, a mismatched pair of private eyes investigate a missing girl and the mysterious death of a porn star. It looks structured and contains all the data we wanted. Now we can start analysing it. 5.4 Performing exploratory data analysis (EDA) on scraped data Let’s explore the IMDB data we scraped. To do so, we will apply some of the principles and techniques for univariate and bivariate data expolation we learnt about in the previous sessions. The first thing we always have to do when analysing data is looking at it. so let’s create a histogram of film duration and add a line representing the average (mean). hist(films$Runtime, breaks=20, col=&quot;grey&quot;, main=&quot;IMDB film duration&quot;, xlab=&quot;Runtime (minutes)&quot;) abline(v=mean(films$Runtime), col=&quot;blue&quot;) We see that film duration is unimodal (with one single bump). Most films last about one and a half hours, while very few last more than 2 and a half hours. Now let’s look at the rating IMDB has assigned to each film. We can visualise this as a stripchart. stripchart(films$IMDB_rating, method = &quot;stack&quot;, pch=19, main=&quot;IMDB film rating&quot;, xlab=&quot;Rating&quot;) We almost never see a rating below 5. This makes sense because, in the end, we are only looking at the top 100 films of the year, so perhaps all the lowly rated films did not make it to this list. There are also very few films with a rating higher than 8. Now let’s use ggplot to create a scatter plot of rating and duration. Is there any relation between the duration of a film and how good or badly it is rated? ggplot(data=films, aes(x=Runtime, y=IMDB_rating)) + geom_point() + xlab(&quot;Runtime (minutes)&quot;) + ylab(&quot;Rating&quot;) + theme_bw() Just by eye, we can see little relation between both variables. This also makes sense: knowing how long a film is does not necesarilly tell us anything about how good or bad it is. Let’s corroborate this by alculating the correlation between both variables. cor(films$Runtime, films$IMDB_rating) ## [1] 0.2957725 The correlation is indeed very low, but it is still possitive! This means that perhaps, even though we cannot really see this correlation by eye, longer films tend to get ranked slightly better. Let’s find which are the top 10 films of 2016 by IMDB rating: top10films &lt;- films[order(-films$IMDB_rating),][1:10,] top10films ## Title Genre Runtime Rank IMDB_rating ## 25 Your Name. Animation 106 25 8.4 ## 8 Hacksaw Ridge Biography 139 8 8.1 ## 13 The Handmaiden Drama 145 13 8.1 ## 92 A Silent Voice Animation 130 92 8.1 ## 1 Deadpool Action 108 1 8.0 ## 5 La La Land Comedy 128 5 8.0 ## 19 Zootropolis Animation 108 19 8.0 ## 46 Lion Biography 118 46 8.0 ## 71 The Invisible Guest Crime 106 71 8.0 ## 10 Arrival Drama 116 10 7.9 ## Description ## 25 Two strangers find themselves linked in a bizarre way. When a connection forms, will distance be the only thing to keep them apart? ## 8 World War II American Army Medic Desmond T. Doss, who served during the Battle of Okinawa, refuses to kill people and becomes the first man in American history to receive the Medal of Honor without firing a shot. ## 13 A woman is hired as a handmaiden to a Japanese heiress, but secretly she is involved in a plot to defraud her. ## 92 A young man is ostracized by his classmates after he bullies a deaf girl to the point where she moves away. Years later, he sets off on a path for redemption. ## 1 A wisecracking mercenary gets experimented on and becomes immortal but ugly, and sets out to track down the man who ruined his looks. ## 5 While navigating their careers in Los Angeles, a pianist and an actress fall in love while attempting to reconcile their aspirations for the future. ## 19 In a city of anthropomorphic animals, a rookie bunny cop and a cynical con artist fox must work together to uncover a conspiracy. ## 46 A five-year-old Indian boy is adopted by an Australian couple after getting lost hundreds of kilometers from home. 25 years later, he sets out to find his lost family. ## 71 A successful entrepreneur accused of murder and a witness preparation expert have less than three hours to come up with an impregnable defense. ## 10 A linguist works with the military to communicate with alien lifeforms after twelve mysterious spacecraft appear around the world. Now let’s add their name labels to the previous plot using geom_text_repel(). ggplot(data=films, aes(x=Runtime, y=IMDB_rating)) + geom_point(size=2) + geom_text_repel(data=top10films, aes(label=Title)) + xlab(&quot;Runtime (minutes)&quot;) + ylab(&quot;Rating&quot;) + theme_bw() We can even colour code the films by genre. ggplot(data=films, aes(x=Runtime, y=IMDB_rating)) + geom_point(aes(color=Genre),size=2) + geom_text_repel(data=top10films, aes(label=Title)) + xlab(&quot;Runtime (minutes)&quot;) + ylab(&quot;Rating&quot;) + theme_bw() ## Warning: ggrepel: 1 unlabeled data points (too many overlaps). Consider increasing max.overlaps However, it is very difficult to distinguis the different colours. In general, it’s not a good idea to display three variables at the same time if we do not have to. For now, let’s focus only on the IMDB ranking and the genre. We can create box plots to compare the ranking of each group: ggplot(data=films, aes(x=Genre, y=IMDB_rating)) + geom_boxplot() + xlab(&quot;Genre&quot;) + ylab(&quot;Rating&quot;) + theme_bw() However note that now we are losing a lot of valuable information on what the individual ratings are. This is because box plots then to “over summarise” the data. Let’s use instead a better alternative called “violin plot”, which is a way to compare density plots for multiple groups. ggplot(data=films, aes(x=Genre, y=IMDB_rating)) + geom_violin(fill=&quot;light blue&quot;) + xlab(&quot;Genre&quot;) + ylab(&quot;Rating&quot;) + theme_bw() We can even add the individual points to ech of this plots too: ggplot(data=films, aes(x=Genre, y=IMDB_rating)) + geom_violin(fill=&quot;light blue&quot;) + geom_jitter(width=0.2) + xlab(&quot;Genre&quot;) + ylab(&quot;Rating&quot;) + theme_bw() Now we see that there is one Crime film with very poor rating compared to the rest. Other than that, all genres seem to be more or less equally popular. Let’s repeat the same procedure, but now analysing the duration of each film stratified by genre. ggplot(data=films, aes(x=Genre, y=Runtime)) + geom_violin(fill=&quot;light blue&quot;) + geom_jitter(width=0.2) + xlab(&quot;Genre&quot;) + ylab(&quot;Rating&quot;) + theme_bw() Now we find something interesting: most of the animation films are much shorter than films of any other genre. In fact, none of them lasts ore than an hour and a half. Some action films, on the other hand, go up to 2.5 hours. This makes sense too, since most animation films are designed for kids, who cannot possibly pay attention to a 2 or 3-hour long film. Finally, let’s use the function “wordcloud()” to find out which are the most commonly used words to describe these 100 films. For a word to be included in the cloud it has to appear at least 3 times in the description. The size of each word is proportional to the number of times it appears. wordcloud(films$Description, scale=c(3,0.3)) ## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): transformation drops documents ## Warning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x, tm::stopwords())): transformation drops documents We see that a lot of films are described using words such as “world”, “must” and “new”. Now you know how to perform web scraping with R and how to retrieve specific data from a website for further analysis. 5.5 Exercises If you want to obtain more web scraping experience, you can do the following: Visit your favourite website Use the Selector Gadget to retrieve the CSS selector for different sections of the site Use R and the CSS selector to scrap the data that most interests you Transform the variables to the appropriate format in R Visualise them. What can you learn from this data? 5.6 References Kaushik S. (March 27, 2017). Beginner’s guide on web scraping in R using rvest. Available from: https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/ "],["machine-learning-fundamentals-spam-detection-using-a-naive-bayes-classifier.html", "Chapter 6 Machine Learning Fundamentals: Spam Detection Using a Naive Bayes Classifier 6.1 Visualising data 6.2 Pre-processing data 6.3 Predicting spam and non-spam 6.4 Discussion 6.5 References", " Chapter 6 Machine Learning Fundamentals: Spam Detection Using a Naive Bayes Classifier In the previous sessions we learnt how to read data (from files or from public websites) into R, how to process it, explore it and perform some basic analysis for one, two or multiple variables. Now that you know how to do all of this, you are prepared to look at the final steps of data analysis: using the data to find new knowledge and apply it to relevant objectives such as finding patterns or predicting outcomes. This falls under the scope of machine learning, a discipline which combines statistics, mathematics and computer science to make sense of data. The two most tasks in types of machine learning are: Clustering: Finding if any patterns or subgroups exist in our data. We learnt a bit of this in our previous session on heatmaps and dendrograms. Classification: Clasifying an individual observation into one of the possible groups of classes. There are multiple applications of this in the real world. For example finding if a bank transaction is normal or fraud, determining if a message is normal or spam, finding whether a star contains exoplanets or not, classifying cancer patients into different types of cancer, etc… In this session we will use a specific type of machine learning called “Naive Bayes” to classify SMS messages into normal or spam. Before starting this session we need to install several R libraries. Some of these libraries are designed to work with text data (remember messages are by definition in a text format) and others contain machine learning functions. To install these libraries, simply run the following lines of code. install.packages(&quot;tm&quot;) install.packages(&quot;e1071&quot;) install.packages(&quot;gmodels&quot;) install.packages(&quot;rlang&quot;) Let’s now load the libraries. library(tm) library(wordcloud) library(rlang) library(e1071) library(gmodels) library(rafalib) You can find the data for this session in the “Data” directory, along with the other materials for this course. The file is called “sms_spam.csv”. Let’s read this into R. Note that I am asking R not to transform text into factors, because messages are mostly text. sms_raw &lt;- read.csv(&quot;./Data/sms_spam.csv&quot;, stringsAsFactors = F) We use the function str() to find the structure of this data str(sms_raw) ## &#39;data.frame&#39;: 5559 obs. of 2 variables: ## $ type: chr &quot;ham&quot; &quot;ham&quot; &quot;ham&quot; &quot;spam&quot; ... ## $ text: chr &quot;Hope you are having a good week. Just checking in&quot; &quot;K..give back my thanks.&quot; &quot;Am also doing in cbe only. But have to pay.&quot; &quot;complimentary 4 STAR Ibiza Holiday or £10,000 cash needs your URGENT collection. 09066364349 NOW from Landline &quot;| __truncated__ ... R tells us that we have two variables: A series of 5559 text (SMS) messages A list which tells us which of these messges are spam and which are normal (“ham”). This has been manually determined by the people who collected the data. Let’s transform the spam/ham labels into a factor, since these really are two categories. sms_raw$type &lt;- factor(sms_raw$type) Now let’s count how many spam and how many “ham” messges there are: table(sms_raw$type) ## ## ham spam ## 4812 747 The majority of them (~ 86%) are normal messages, and only 14% are spam. 6.1 Visualising data Let’s now visualise the data. Because this data is in a text format, we cannot create histograms or scatter plots for it. Instead, let’s use a word cloud to find which words are used most frequently. Because there are almost 6 thousand messages and thosands of words, we ask R to only show us the 50 most common words. wordcloud(sms_raw$text, max.words = 50, random.order = F) Now, let’s use the function subset() to separate the data in two: a list of normal messages and a list of spam messages. spam &lt;- subset(sms_raw, type==&quot;spam&quot;) ham &lt;- subset(sms_raw, type==&quot;ham&quot;) Let’s now repeat the word cloud separately for each group. In this way, we can compare the word composition of spam and non-spam messages. mypar(1,2) wordcloud(spam$text, max.words = 50, scale=c(3,0.5), random.order = F) wordcloud(ham$text, max.words = 50, scale=c(2,0.5), random.order = F) We can already see some major differences between both types of message. Spam messages use words as “call”, “free”, “reply” and even “£200” (beause what they want is for you to call them back or to make you think you’ve won something). Normal messges between humans talk about much more emotional matters. For example, in normal messages we see words like “can”, “know”, “sorry”, “good” or even “home”. If these messages are so different, can we use their word composition to separate them or create a spam filter? We will try to apply machine learning to achieve this. 6.2 Pre-processing data The first thing we need to do is putting the data in the right format. Machine learning uses statistics, and statistics can only deal with numbers or proportions. Thus, we will have to convert these messages into a quantitative format. 6.2.1 Converting data to corpus We start by converting our 6000 messages into a “corpus”. A corpus is a list, where each element of the list is a series of words. We use the function Corpus(). sms_corpus &lt;- Corpus(VectorSource(sms_raw$text)) We can use the function “inspect()” to look at the first three messages in the corpus: inspect(sms_corpus[1:3]) ## &lt;&lt;SimpleCorpus&gt;&gt; ## Metadata: corpus specific: 1, document level (indexed): 0 ## Content: documents: 3 ## ## [1] Hope you are having a good week. Just checking in K..give back my thanks. Am also doing in cbe only. But have to pay. 6.2.2 Cleaning data Now we need to clean the data and remove all the noise and unnecessary words. There can be a lot of unnecessary words and signs in 6 thousand messages! The first problem with words is that R is case sensitive, so words starting with upper case or lower case will be seen by R as different things, even if they are the exact same word. Thus, let’s transf all the words to lower case using the tm_map() function and specifying we want lower case. We’ll store our clean body of words in the variable “corpus_clean”. corpus_clean &lt;- tm_map(sms_corpus, tolower) Next, let’s remove all the numbers and keep only words. corpus_clean &lt;- tm_map(corpus_clean, removeNumbers) Some words are absolutely useless for classifying messages. For example, stop words such as “to”,“and”,“but”,“or”, etc… give us very little information and usually appear a lot of times, hence taking up too much memory. Let’s use tm_map() to remove all stop words. corpus_clean &lt;- tm_map(corpus_clean, removeWords, stopwords()) Now let’s also remove all the punctuation marks (dots, commas, quotes, etc…). corpus_clean &lt;- tm_map(corpus_clean, removePunctuation) Finally, let’s remove the extra white space, because often people make typos when composing messages and hit the space key multiple times. corpus_clean &lt;- tm_map(corpus_clean, stripWhitespace) Let’s have a look at our final, clean set of words. inspect(corpus_clean[1:5]) ## &lt;&lt;SimpleCorpus&gt;&gt; ## Metadata: corpus specific: 1, document level (indexed): 0 ## Content: documents: 5 ## ## [1] hope good week just checking ## [2] kgive back thanks ## [3] also cbe pay ## [4] complimentary star ibiza holiday £ cash needs urgent collection now landline lose boxskwpppm ## [5] okmail dear dave final notice collect tenerife holiday cash award call landline tcs sae box cwwx ppm Note how a lot of words dissappeared and now all the letters are lower case. 6.2.3 Visualising clean data Let’s create a new word cloud, but now using the clean data. wordcloud(corpus_clean, min.freq = 40, scale=c(3,0.5), random.order = F) 6.2.4 Tokenising Now comes the most important part: transforming our text data to something quantitative (ie. numbers). The way we do this is by counting how many times each word appears in each message. This will result in a “word count matrix”. In other words, we will construct a matrix in which every row is a message and every column is a word. Each element of the matrix is the number of times that word appeared in this message. These type of matrix is sometimes called “document term matrix”. Let’s use R’s function DocumentTermMatrix() to convert our clean data into numbers. This processed is often referred to as “tokenising”. sms_dtm &lt;- DocumentTermMatrix(corpus_clean) 6.2.5 Dividing data into training and testing set Now we are ready to start classifying the messages. As you may have learnt in the theoretical session, the way machine learning work is the following: The algorithm is “trained” on a very big data set. Using this data, it will find the patterns that characterise spam and normal messages The algorithm is applied to new (test) data and classifies each new message as normal or spam. We can use these results to analyse how the algorithm performed. Let’s first divide the messages in two groups: training and test. We do this on the raw data, the corpus and the word count matrix. Training data set: This set will contain 75% of the messages (messages 1 to 4169): sms_raw_train &lt;- sms_raw[1:4169,] sms_corpus_train &lt;- corpus_clean[1:4169] sms_dtm_train &lt;- sms_dtm[1:4169,] Test data set: This set will contain 25% of the messages (messages 4170 to 5559): sms_raw_test &lt;- sms_raw[4170:5559,] sms_corpus_test &lt;- corpus_clean[4170:5559] sms_dtm_test &lt;- sms_dtm[4170:5559,] 6.2.6 Correcting data sparsity There is one last problem we need to solve before building our machine learning model. Because so many words exist in the English languge (just look at the size of any dictionary!), chances are there will be a lot of words that only appear in one, two or three of the messages. Theses words are not useful: they will not help us differentiate spam from normal. In statistics this is called “sparsity”: when a very big part of a matrix is full of zeros (in this case, words that appear zero times in most messages). We should remove this sparsity. To do that, let’s restrict our data to frequent words only. All words that appear in less than 5 messages will be discarded. Frequent words can be found using the function findFreqTerms(). sms_dict &lt;- findFreqTerms(sms_dtm_train, 5) Let’s only keep these frequent words sms_train &lt;- DocumentTermMatrix(sms_corpus_train, list(dictionary=sms_dict)) sms_test &lt;- DocumentTermMatrix(sms_corpus_test, list(dictionary=sms_dict)) 6.2.7 Converting data to word appearance table At this point, we have create a matrix with words as columns and messages as rows. However, to make things simpler we will restrict our matrix to only two values: if a word appears inside a message the entry is set to “Yes” (1). If it doesn’t, to “No” (0). This function will convert the word counts to YES and NOs. convert_counts &lt;- function(x){ x &lt;- ifelse(x &gt; 0, 1, 0) x &lt;- factor(x, levels=c(0,1), labels=c(&quot;No&quot;,&quot;Yes&quot;)) return(x) } Let’s apply it to both our training and our test data. sms_train &lt;- apply(sms_train, MARGIN = 2, convert_counts) sms_test &lt;- apply(sms_test, MARGIN = 2, convert_counts) Finally, let’s have a look at the data. sms_train[1:10,1:10] ## Terms ## Docs checking good hope just week back thanks also pay cash ## 1 &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ## 2 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ## 3 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;No&quot; ## 4 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;Yes&quot; ## 5 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;Yes&quot; ## 6 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ## 7 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ## 8 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ## 9 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ## 10 &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; Now we have a matrix of Y and N which tells us which message contains which word. There are 1218 words in this matrix, so you can think of it as a space with about 1200 dimensions (each word is a dimension)! Now we are ready to do machine learning! 6.3 Predicting spam and non-spam 6.3.1 Training classifier We use the YES/NO data to “train” a classifier. The type of classifier we will work with in this session is called “Naive Bayes”. The way this algorithm works involves advanced statistics. Some of this will be explained during the theoretical session. However, in general terms we find out which words are contianed in a message and which words are not, based on this evidence we calculate the probability that this message is spam or normal. If this probability is high, we say that the message is spam. If it is low, we say that the message is normal. We repeat this for every messag in the data set. Let’s use the training data to “train” our naive bayes algorithm. All you need to do is use the naiveBayes function as follows: sms_classifier &lt;- naiveBayes(sms_train, sms_raw_train$type) Now the algorithm has been trained. In other words, it has looked at the data and found which words appear in spam and in normal messages, then it has calculated probabilities for each of them. 6.3.2 Testing classifier Now we can apply the algorithm we just trained on the remaining 25% of the data (the test set). Is our clasifier able to predict which messages are spam and which are not? To make this prediction we simply use the function “predict” followed by the variable in which our trained model is stored. This step can take a few seconds to run. After all, R is using thousands of words to predict if each message is or isn’t spam! sms_predicted &lt;- predict(sms_classifier, sms_test) 6.3.3 Evaluating performance Let’s find out how good our predictions were. We use the function CrossTable to compare the actual class of each message (spam or normal, as determined manually by reserachers) with the class we predicted them to have. CrossTable(sms_predicted, sms_raw_test$type, prop.chisq = FALSE, prop.t =FALSE, dnn = c(&quot;predicted&quot;,&quot;actual&quot;)) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Row Total | ## | N / Col Total | ## |-------------------------| ## ## ## Total Observations in Table: 1390 ## ## ## | actual ## predicted | ham | spam | Row Total | ## -------------|-----------|-----------|-----------| ## ham | 1203 | 32 | 1235 | ## | 0.974 | 0.026 | 0.888 | ## | 0.997 | 0.175 | | ## -------------|-----------|-----------|-----------| ## spam | 4 | 151 | 155 | ## | 0.026 | 0.974 | 0.112 | ## | 0.003 | 0.825 | | ## -------------|-----------|-----------|-----------| ## Column Total | 1207 | 183 | 1390 | ## | 0.868 | 0.132 | | ## -------------|-----------|-----------|-----------| ## ## You can see that 99.7% of normal messages were detected by our fileter as normal. Only 4 out of more than one thousand normal messags were misclassified as spam. This is a really good performance. On the other hand, about 82.5% of spam messages were correctly labeled as spam. Approximately 32 out of 180 spam messages were misclassified as normal. This performance is good, but could definitely be improved! Now you know a bit more about how machine learning is used with different comercial and scientific purposes. In this case, to build a spam filter. There are many more machine learning techniques, some of them incredibly accurate and complex. They can be (and in fact) are applied all the time to solve all sorts of problems. Examples of this are recommendations displayed to you on Google, Facebook or Amazon; studying which regions of the brain are active when people perform different tasks; classifying cancers into groups; or performing language processing (for example in devices capable of translating from one language to another in real time). 6.4 Discussion Can you think of any other applications of machine learning? In which ways do you think machine learning is changing our daily lives? Are these changes positive or negative? What are their implications? 6.5 References This turotial was adapted from: Lantz B. (2013). Chapter 4. Probabilistic learning and classification using Naive Bayes. In Machine Learning with R. Birmingham: Packt Publishing. SMS data comes from the study: Gomez-Hidalgo JM, Almeida TA and Yamakami A. (2012). On the validity of a new SMS spam collection. Proceedings of the 11th international conference on machine learning and applications. "],["machine-learning-fundamentals-predicting-variables-using-neural-networks.html", "Chapter 7 Machine Learning Fundamentals: Predicting Variables Using Neural Networks 7.1 Normalising data 7.2 Visualising data 7.3 Creating neural network 7.4 Conclusions 7.5 References", " Chapter 7 Machine Learning Fundamentals: Predicting Variables Using Neural Networks In the previous sessions we learnt how to read and explore data with multiple variables/dimensions in R, as well as how to build a spam filter using machine learning (Naive Bayes). Finally, we will learn how to build a simple neural network using R. Recall that neural networks are machine learning algorithms inspired by the biology of the brain and which can be used for the two following tasks: Prediction: Using a set of independent variables to “predict” a dependent variable (just as in linear regression, only with multiple variables). Classification: Clasifying an individual observation into one of the possible groups of classes. In this session we will build a neural network to predict the strength of concrete. To do so, we will use data from 1030 different types of concrete, each with a different age and composition. Before starting this session we need to install a new R libraries called “neuralnet.” To install this library, simply run the following line of code. install.packages(&quot;neuralnet&quot;) Let’s now load the required libraries. library(neuralnet) library(pheatmap) library(corrplot) library(ggplot2) You can find the data for this session in the “Data” directory, along with the other materials for this course. The file is called “concrete.csv”. Let’s read this into R. concrete &lt;- read.csv(&quot;./Data/concrete.csv&quot;) We use the head function to look at the first 6 lines of the table. head(concrete) ## cement slag ash water superplastic coarseagg fineagg age strength ## 1 141.3 212.0 0.0 203.5 0.0 971.8 748.5 28 29.89 ## 2 168.9 42.2 124.3 158.3 10.8 1080.8 796.2 14 23.51 ## 3 250.0 0.0 95.7 187.4 5.5 956.9 861.2 28 29.22 ## 4 266.0 114.0 0.0 228.0 0.0 932.0 670.0 28 45.85 ## 5 154.8 183.4 0.0 193.3 9.1 1047.4 696.7 28 18.29 ## 6 255.0 0.0 0.0 192.0 0.0 889.8 945.0 90 21.86 We can see that this data contains measurements on the following characteristics of concrete: Amount of cement (kg/m3) Amount of slag (kg/m3) Amount of ash (kg/m3) Amount of water (kg/m3) Amount of superplasticizer (kg/m3) Amount of coarse aggregate (kg/m3) Amount of fine aggregate (kg/m3) Aging time (days) Compressive strength Let’s now use the dim() function to find out how many samples of concrete were measured: dim(concrete) ## [1] 1030 9 Our data contains measurements on 1,030 different samples of concrete. 7.1 Normalising data Before analysing the data, let’s use the summary() function to find out how each variable behaves. summary(concrete) ## cement slag ash water superplastic coarseagg fineagg age strength ## Min. :102.0 Min. : 0.0 Min. : 0.00 Min. :121.8 Min. : 0.000 Min. : 801.0 Min. :594.0 Min. : 1.00 Min. : 2.33 ## 1st Qu.:192.4 1st Qu.: 0.0 1st Qu.: 0.00 1st Qu.:164.9 1st Qu.: 0.000 1st Qu.: 932.0 1st Qu.:731.0 1st Qu.: 7.00 1st Qu.:23.71 ## Median :272.9 Median : 22.0 Median : 0.00 Median :185.0 Median : 6.400 Median : 968.0 Median :779.5 Median : 28.00 Median :34.45 ## Mean :281.2 Mean : 73.9 Mean : 54.19 Mean :181.6 Mean : 6.205 Mean : 972.9 Mean :773.6 Mean : 45.66 Mean :35.82 ## 3rd Qu.:350.0 3rd Qu.:142.9 3rd Qu.:118.30 3rd Qu.:192.0 3rd Qu.:10.200 3rd Qu.:1029.4 3rd Qu.:824.0 3rd Qu.: 56.00 3rd Qu.:46.13 ## Max. :540.0 Max. :359.4 Max. :200.10 Max. :247.0 Max. :32.200 Max. :1145.0 Max. :992.6 Max. :365.00 Max. :82.60 Note that while the amount of cement can go from 102 to 540 kg/m3, the amount of coarse aggregate can go from 801 to 1029 kg/m3. These ranges are very different to each other, so we cannot compare simply compare both measurements! Moreover, NEURAL NETWORKS ONLY WORK ON VALUES CLOSE TO ZERO. Because of these two reasons, let’s “rescale” the values of all columns, so that they go from 0 to 1 only. This process is called “rescaling” or “normalisation”. We define a function called “normalisation”, which takes a series of numbers (x) and normalises it, so that the numbers go from 0 to 1 only. The function finally resturns all of these “new x” values. normalise &lt;- function(x){ new.x &lt;- (x -min(x))/(max(x)-min(x)) } Next, we apply this function to all the columns of our data. We do this using “apply”. We set the MARGIN to 2, because we want to apply the function to the columns. We store the new data in a variable called “concrete_norm” (norm for normalised). concrete_norm &lt;- as.data.frame(apply(concrete, MARGIN=2, normalise)) Let’s use head() to look at the first lines of the data. head(concrete_norm) ## cement slag ash water superplastic coarseagg fineagg age strength ## 1 0.08972603 0.5898720 0.0000000 0.6525559 0.0000000 0.4965116 0.3876066 0.07417582 0.3433412 ## 2 0.15273973 0.1174179 0.6211894 0.2915335 0.3354037 0.8133721 0.5072755 0.03571429 0.2638595 ## 3 0.33789954 0.0000000 0.4782609 0.5239617 0.1708075 0.4531977 0.6703462 0.07417582 0.3349944 ## 4 0.37442922 0.3171953 0.0000000 0.8482428 0.0000000 0.3808140 0.1906673 0.07417582 0.5421702 ## 5 0.12054795 0.5102949 0.0000000 0.5710863 0.2826087 0.7162791 0.2576518 0.07417582 0.1988290 ## 6 0.34931507 0.0000000 0.0000000 0.5607029 0.0000000 0.2581395 0.8805820 0.24450549 0.2433038 We see that now all values are between zero and one. To verify that this is the case, let’s use the summary function. summary(concrete_norm) ## cement slag ash water superplastic coarseagg fineagg age strength ## Min. :0.0000 Min. :0.00000 Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.00000 Min. :0.0000 ## 1st Qu.:0.2063 1st Qu.:0.00000 1st Qu.:0.0000 1st Qu.:0.3442 1st Qu.:0.0000 1st Qu.:0.3808 1st Qu.:0.3436 1st Qu.:0.01648 1st Qu.:0.2664 ## Median :0.3902 Median :0.06121 Median :0.0000 Median :0.5048 Median :0.1988 Median :0.4855 Median :0.4654 Median :0.07418 Median :0.4001 ## Mean :0.4091 Mean :0.20561 Mean :0.2708 Mean :0.4774 Mean :0.1927 Mean :0.4998 Mean :0.4505 Mean :0.12270 Mean :0.4172 ## 3rd Qu.:0.5662 3rd Qu.:0.39775 3rd Qu.:0.5912 3rd Qu.:0.5607 3rd Qu.:0.3168 3rd Qu.:0.6640 3rd Qu.:0.5770 3rd Qu.:0.15110 3rd Qu.:0.5457 ## Max. :1.0000 Max. :1.00000 Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.00000 Max. :1.0000 Indeed, notice how the minimum and maximum values are 0 and 1 respectively for ALL columns in our table. Now we are ready to analyse the data. 7.2 Visualising data Before building our neural network, let’s use the techniques we have learnt before to explore the concrete data. For example, let’s calculate the correlation matrix. corrmat &lt;- cor(concrete_norm) Let’s use corrplot() to visualise the correlation matrix. We will ask R to plot the most similar variables together (recall that we can do this by setting order=“hclust”). corrplot(corrmat, type = &quot;upper&quot;, method = &quot;ellipse&quot;, order = &quot;hclust&quot;) We can already conclude some things from this graph. For example, the strength of concrete is highly correlated with the amount of cement we put in it. Thus, if we want to build stronger concrete we might need to add more cement. We can also see that the amount of superplasticizer is inversely proportional to the aging time. This might mean that as concrete ages, the plasticizer is degraded and disappears. Let’s now use the data to build a heat map. pheatmap(concrete_norm, show_rownames = F) From this graph we get even further information. For instance, notice how there are at least 6 different “types” of concrete (you can see this in the way rows have been ordered into a tree). Finally, let’s try to reduce the dimensions of the data using PCA. pcs &lt;- prcomp(concrete_norm) A closer look at PC1 and PC2 tells us that one of the main differences between different concretes is the amount of ash. In fact, PC1 (our most important new axis) separates concretes into those with low ash content and those with high ash content. ggplot(data.frame(pcs$x), aes(PC1, PC2)) + geom_point(aes(color=concrete_norm$ash)) + scale_color_continuous(name=&quot;Ash&quot;) + theme_bw() We have learnt a lot from the data already, but we still do not know how to predict its strength. If we know the composition of a concrete sample, can we predict how strong it will be and, thus, which things we should and should not build with it? Let’s build a neural network to answer this question. 7.3 Creating neural network Remember that machine learning is done in two steps: training and testing. Thus, let’s first divide the data into a training and a testing set. 7.3.1 Spliting data into training and testing sets We will allocate 75% of our observations (773 in total) to the training data set and the remaining 25% (257) to the testing data set. concrete_training &lt;- concrete_norm[1:773,] concrete_testing &lt;- concrete_norm[773:1030,] 7.3.2 Training neural network Now we are able to build and test the neural network. To do this, we will use the neuralnet() function, specifying which are the independent and which the dependent variables (into a formula). For now, let’s build the most simple possible network: one with just one hidden node! my_NN &lt;- neuralnet(strength ~ cement + slag + ash + water + superplastic + coarseagg + fineagg + age, data = concrete_training, hidden = 1) Our network has been trained! Let’s have a look at its shape (its topology) using the plot function. plot(my_NN) Notice that it more than 1,000 iterations to build this network, despite it being so simple. 7.3.3 Testing neural network Now we can test the performance of the network. We use the compute() function to try and predict the strength of concrete in the testing set. results &lt;- compute(my_NN, concrete_testing[,1:8]) Let’s use head() to look at the first 6 predictions done by our network. head(results$net.result) ## [,1] ## 773 0.1940853 ## 774 0.3263611 ## 775 0.4671574 ## 776 0.2372741 ## 777 0.6722401 ## 778 0.4648155 7.3.4 Evaluating performance Finally, let’s evaluate how well these predictions are by comparing them with the true concrete strength. This is possible because we knew the true values from the beginning! Let’s build a scatter plot of prediciton versus reality. plot(concrete_testing$strength, results$net.result, main = &quot;Artificial NN with 3 layers and 1 hidden node&quot;, xlab=&quot;True concrete strength&quot;, ylab=&quot;Predicted concrete strength&quot;) abline(lm(results$net.result ~ concrete_testing$strength)) Notice how well the network perform, despite being so simple! In fact, if we calculate the correlation we’ll see that it is higher than 0.8 cor(concrete_testing$strength, results$net.result) ## [,1] ## [1,] 0.8070362 7.3.5 Improving performance by adding hidden nodes However, these predictions are still not good enough. What if we wanted to evaluate which concrete is the best for a really important bridge? We cannot risk overestimating the strength and making a bridge collapse! Thus, let’s try to improve our predictions. To do this, we will recreate the network, but now making it much more complex. In fact, let’s add 6 hidden nodes to the middle layer. my_NN &lt;- neuralnet(strength ~ cement + slag + ash + water + superplastic + coarseagg + fineagg + age, data = concrete_training, hidden = 6) The new topology of the network looks as follows: plot(my_NN) Note how it took more than 50,000 iterations to build this network configuration!!! Let’s predict the concrete strengths again, using the new neural network. results &lt;- compute(my_NN, concrete_testing[,1:8]) To evaluate the performance, let’s plot the predicted values versus the true values. plot(concrete_testing$strength, results$net.result, main = &quot;Artificial NN with 3 layers and 6 hidden nodes&quot;, xlab=&quot;True concrete strength&quot;, ylab=&quot;Predicted concrete strength&quot;) abline(lm(results$net.result ~ concrete_testing$strength)) The points fall in an almost perfect line! In fact, if we calculate the correlation we will notice that it has improved to more than 0.9. cor(concrete_testing$strength, results$net.result) ## [,1] ## [1,] 0.9350887 This means that our neural network correctly predicts the strength of concrete using these 8 variables. We can now be confident enough to use it in new data. 7.4 Conclusions Now you know how to build simple neural networks in R. You can apply this knowledge to predict or classify any data you might be interested in. 7.5 References This turotial was adapted from: Lantz B. (2013). Chapter 7. Neural Networks and Support Vector Machines in Machine Learning with R. Birmingham: Packt Publishing. Concrete strength data comes from the following study: Yeh IC. (1998). Modelling of strength of high performance concrete using artificial neural networks. Cemenr and Concrete Research. Vol 28. pp 1797-1808. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
